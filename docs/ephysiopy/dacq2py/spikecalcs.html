<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.5" />
<title>ephysiopy.dacq2py.spikecalcs API documentation</title>
<meta name="description" content="A lot of the functionality here has been more generically implemented
in the ephys_generic.ephys_generic.SpikeCalcsGeneric class" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ephysiopy.dacq2py.spikecalcs</code></h1>
</header>
<section id="section-intro">
<p>A lot of the functionality here has been more generically implemented
in the ephys_generic.ephys_generic.SpikeCalcsGeneric class</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
A lot of the functionality here has been more generically implemented
in the ephys_generic.ephys_generic.SpikeCalcsGeneric class
&#34;&#34;&#34;
import numpy as np
import warnings
from scipy import signal                
from scipy import stats
import matplotlib.pyplot as plt
from matplotlib import colors
from .utils import blur_image

class SpikeCalcs(object):
        &#34;&#34;&#34;
        Mix-in class for use with Tetrode class below.
        
        Extends Tetrodes functionality by adding methods for analysis of spikes/
        spike trains
                                
        Note lots of the methods here are native to dacq2py.axonaIO.Tetrode
        
        Note that units are in milliseconds
        &#34;&#34;&#34;
        
        
        def getNSpikes(self, cluster):
                if cluster not in self.clusters:
                        warnings.warn(&#39;Cluster not available. Try again!&#39;)
                else:
                        return np.count_nonzero(self.cut == cluster)

        def trial_av_firing_rate(self, cluster):
                &#39;&#39;&#39;
                returns the trial average firing rate of a cluster in Hz
                &#39;&#39;&#39;
                return self.getNSpikes(cluster) / float(self.header[&#39;duration&#39;])
        
        def mean_autoCorr(self, cluster, n=40):
                &#39;&#39;&#39;
                Returns the autocorrelation function mean from 0 to n ms (default=40)
                Used to help classify units as principal or interneuron
                &#39;&#39;&#39;
                if cluster not in self.clusters:
                        warnings.warn(&#39;Cluster not available. Try again!&#39;)
                else:
                        bins = 201
                        Trange = (-500, 500)
                        y = self.xcorr(cluster, Trange=Trange)
                        counts, bins = np.histogram(y[y != 0], bins=bins, range=Trange)
                        mask = np.logical_and(bins&gt;0, bins&lt;n)
                        return np.mean(counts[mask])
                
        def ifr_sp_corr(self, clusterA, speed, minSpeed=2.0, maxSpeed=40.0, sigma=3, 
                                        shuffle=False, nShuffles=100, minTime=30, plot=False):
                &#34;&#34;&#34;
                clusterA: int
                        the cluster to do the correlation with speed
                speed: np.array (1 x nSamples)
                        instantaneous speed 
                minSpeed: int
                        speeds below this value are ignored - defaults to 2cm/s as with
                        Kropff et al., 2015
                &#34;&#34;&#34;
                if clusterA not in self.clusters:
                        warnings.warn(&#39;Cluster not available. Try again!&#39;)
                else:
                        speed = speed.ravel()
                        posSampRate = 50
                        nSamples = len(speed)
                        x1 = self.getClustIdx(clusterA)
                        # position is sampled at 50Hz and so is &#39;automatically&#39; binned into
                        # 20ms bins
                        spk_hist = np.bincount(x1, minlength=nSamples)
                        # smooth the spk_hist (which is a temporal histogram) with a 250ms
                        # gaussian as with Kropff et al., 2015
                        h = signal.gaussian(13, sigma)
                        h = h / float(np.sum(h))
                        #filter for low speeds
                        lowSpeedIdx = speed &lt; minSpeed
                        highSpeedIdx = speed &gt; maxSpeed
                        speed_filt = speed[~np.logical_or(lowSpeedIdx, highSpeedIdx)]
                        spk_hist_filt = spk_hist[~np.logical_or(lowSpeedIdx, highSpeedIdx)]
                        spk_sm = signal.filtfilt(h.ravel(), 1, spk_hist_filt)
                        sm_spk_rate = spk_sm * posSampRate
                        
                        res = stats.pearsonr(sm_spk_rate, speed_filt)
                        if plot:            
                                # do some fancy plotting stuff
                                speed_binned, sp_bin_edges = np.histogram(speed_filt, bins=50)
                                sp_dig = np.digitize(speed_filt, sp_bin_edges, right=True)
                                spks_per_sp_bin = [spk_hist_filt[sp_dig==i] for i in range(len(sp_bin_edges))] 
                                rate_per_sp_bin = []
                                for x in spks_per_sp_bin:
                                        rate_per_sp_bin.append(np.mean(x) * posSampRate)
                                rate_filter = signal.gaussian(5, 1.0)
                                rate_filter = rate_filter / np.sum(rate_filter)
                                binned_spk_rate = signal.filtfilt(rate_filter, 1, rate_per_sp_bin)
                                # instead of plotting a scatter plot of the firing rate at each 
                                # speed bin, plot a log normalised heatmap and overlay results on it
                                
                                spk_binning_edges = np.linspace(np.min(sm_spk_rate), np.max(sm_spk_rate),
                                                                                                len(sp_bin_edges))
                                speed_mesh, spk_mesh = np.meshgrid(sp_bin_edges, spk_binning_edges)
                                binned_rate, _, _ = np.histogram2d(speed_filt, sm_spk_rate, bins=[sp_bin_edges,
                                                                                                   spk_binning_edges])
                                #blur the binned rate a bit to make it look nicer
                                sm_binned_rate = blur_image(binned_rate, 5)
                                plt.figure()
                                plt.pcolormesh(speed_mesh, spk_mesh, sm_binned_rate, norm=colors.LogNorm(), alpha=0.5, shading=&#39;flat&#39;, edgecolors=&#39;None&#39;)
                                #overlay the smoothed binned rate against speed
                                plt.hold(True)
                                plt.plot(sp_bin_edges, binned_spk_rate, &#39;r&#39;)
                                #do the linear regression and plot the fit too
                                # TODO: linear regression is broken ie not regressing the correct variables
                                lr = stats.linregress(speed_filt, sm_spk_rate)
                                end_point = lr.intercept + ((sp_bin_edges[-1] - sp_bin_edges[0]) * lr.slope)
                                plt.plot([np.min(sp_bin_edges), np.max(sp_bin_edges)], [lr.intercept, end_point], &#39;r--&#39;)
                                ax = plt.gca()
                                ax.set_xlim(np.min(sp_bin_edges), np.max(sp_bin_edges[-2]))
                                ax.set_ylim(0, np.nanmax(binned_spk_rate) * 1.1)
                                ax.set_ylabel(&#39;Firing rate(Hz)&#39;)
                                ax.set_xlabel(&#39;Running speed(cm/s)&#39;)
                                ax.set_title(&#39;Intercept: {0:.3f}    Slope: {1:.5f}\nPearson: {2:.5f}&#39;.format(lr.intercept, lr.slope, lr.rvalue))
                        #do some shuffling of the data to see if the result is signficant            
                        if shuffle:                
                                # shift spikes by at least 30 seconds after trial start and
                                # 30 seconds before trial end
                                timeSteps = np.random.randint(30 * posSampRate, nSamples - (30 * posSampRate),
                                                                                                          nShuffles)
                                shuffled_results = []            
                                for t in timeSteps:
                                        spk_count = np.roll(spk_hist, t)
                                        spk_count_filt = spk_count[~lowSpeedIdx]
                                        spk_count_sm = signal.filtfilt(h.ravel(), 1, spk_count_filt)
                                        shuffled_results.append(stats.pearsonr(spk_count_sm, speed_filt)[0])
                                if plot:
                                        plt.figure()
                                        ax = plt.gca()
                                        ax.hist(np.abs(shuffled_results), 20)
                                        ylims = ax.get_ylim()
                                        ax.vlines(res, ylims[0], ylims[1], &#39;r&#39;)
                                
                        print(&#34;PPMC: {0}&#34;.format(res[0]))

        def xcorr(self, x1, x2=None, Trange=None):
                &#39;&#39;&#39;
                Returns the histogram of the ISIs

                Parameters
                ---------------
                x1 - 1d np.array list of spike times
                x2 - (optional) 1d np.array of spike times
                Trange - 1x2 np.array for range of times to bin up. Defaults
                                        to [-500, +500]
                &#39;&#39;&#39;
                if x2 is None:
                        x2 = x1.copy()
                if Trange is None:
                        Trange = np.array([-500, 500])
                y = []
                irange = x1[:, np.newaxis] + Trange[np.newaxis, :]
                dts = np.searchsorted(x2, irange)
                for i, t in enumerate(dts):
                        y.extend(x2[t[0]:t[1]] - x1[i])
                y = np.array(y, dtype=float)
                return y

        def smoothSpikePosCount(self, x1, npos, sigma=3.0, shuffle=None):
                &#39;&#39;&#39;
                Returns a spike train the same length as num pos samples that has been
                smoothed in time with a gaussian kernel M in width and standard deviation
                equal to sigma
                
                Parameters
                --------------
                x1 : np.array
                        The pos indices the spikes occured at
                npos : int
                        The number of position samples captured
                sigma : float
                        the standard deviation of the gaussian used to smooth the spike
                        train
                shuffle: int
                        The number of seconds to shift the spike train by. Default None
                
                Returns
                -----------
                smoothed_spikes : np.array
                        The smoothed spike train
                &#39;&#39;&#39;
                spk_hist = np.bincount(x1, minlength=npos)
                if shuffle is not None:
                        spk_hist = np.roll(spk_hist, int(shuffle * 50))
                # smooth the spk_hist (which is a temporal histogram) with a 250ms
                # gaussian as with Kropff et al., 2015
                h = signal.gaussian(13, sigma)
                h = h / float(np.sum(h))
                return signal.filtfilt(h.ravel(), 1, spk_hist)
        
        def getMeanWaveform(self, clusterA):
                &#39;&#39;&#39;
                Returns the mean waveform and sem for a given spike train
                
                Parameters
                ----------
                clusterA: int
                        The cluster to get the mean waveform for
                        
                Returns
                -------
                mn_wvs: ndarray (floats) - usually 4x50 for tetrode recordings
                        the mean waveforms
                std_wvs: ndarray (floats) - usually 4x50 for tetrode recordings
                        the standard deviations of the waveforms
                &#39;&#39;&#39;
                if clusterA not in self.clusters:
                        warnings.warn(&#39;Cluster not available. Try again!&#39;)
                x = self.getClustSpks(clusterA)
                return np.mean(x, axis=0), np.std(x, axis=0)

        def thetaBandMaxFreq(self, x1):
                &#39;&#39;&#39;
                Calculates the frequency with the max power in the theta band (6-12Hz)
                of a spike trains autocorrelogram. Partly to look for differences
                in theta frequency in different running directions a la Blair (Welday paper)
                &#39;&#39;&#39;
                y = self.xcorr(x1)
                corr, _ = np.histogram(y[y != 0], bins=201, range=np.array([-500,500]))
                # Take the fft of the spike train autocorr (from -500 to +500ms)
                from scipy.signal import periodogram
                freqs, power = periodogram(corr, fs=200, return_onesided=True)
                power_masked = np.ma.MaskedArray(power,np.logical_or(freqs&lt;6,freqs&gt;12))
                return freqs[np.argmax(power_masked)]

        def thetaModIdx(self, x1):
                &#39;&#39;&#39;
                Calculates a theta modulation index of a spike train based on the cells
                autocorrelogram
                
                Parameters
                ----------
                x1: np.array
                        The spike time-series
                Returns
                -------
                thetaMod: float
                        The difference of the values at the first peak and trough of the
                        autocorrelogram
                &#39;&#39;&#39;
                y = self.xcorr(x1)
                corr, _ = np.histogram(y[y != 0], bins=201, range=np.array([-500,500]))
                # Take the fft of the spike train autocorr (from -500 to +500ms)
                from scipy.signal import periodogram
                freqs, power = periodogram(corr, fs=200, return_onesided=True)
                # Smooth the power over +/- 1Hz
                b = signal.boxcar(3)
                h = signal.filtfilt(b, 3, power)
                
                # Square the amplitude first
                sqd_amp = h ** 2
                # Then find the mean power in the +/-1Hz band either side of that
                theta_band_max_idx = np.nonzero(sqd_amp==np.max(sqd_amp[np.logical_and(freqs&gt;6, freqs&lt;11)]))[0][0]
                mean_theta_band_power = np.mean(sqd_amp[theta_band_max_idx-1:theta_band_max_idx+1])
                # Find the mean amplitude in the 2-50Hz range
                other_band_idx = np.logical_and(freqs&gt;2, freqs&lt;50)
                mean_other_band_power = np.mean(sqd_amp[other_band_idx])
                # Find the ratio of these two - this is the theta modulation index
                return (mean_theta_band_power - mean_other_band_power) / (mean_theta_band_power + mean_other_band_power)

        def thetaModIdxV2(self, x1):
                &#39;&#39;&#39;
                This is a simpler alternative to the thetaModIdx method in that it
                calculates the difference between the normalized temporal autocorrelogram
                at the trough between 50-70ms and the peak between 100-140ms over
                their sum (data is binned into 5ms bins)
                
                Measure used in Cacucci et al., 2004 and Kropff et al 2015
                &#39;&#39;&#39;
                y = self.xcorr(x1)
                corr, bins = np.histogram(y[y != 0], bins=201, range=np.array([-500,500]))
                # &#39;close&#39; the right-hand bin
                bins = bins[0:-1]
                # normalise corr so max is 1.0
                corr = corr/float(np.max(corr))
                thetaAntiPhase = np.min(corr[np.logical_and(bins&gt;50,bins&lt;70)])
                thetaPhase = np.max(corr[np.logical_and(bins&gt;100, bins&lt;140)])
                return (thetaPhase-thetaAntiPhase) / (thetaPhase+thetaAntiPhase)

        def clusterQuality(self, cluster, fet=1):
                &#39;&#39;&#39;
                returns the L-ratio and Isolation Distance measures
                calculated on the principal components of the energy in a spike matrix
                &#39;&#39;&#39;
                nSpikes, nElectrodes, _ = self.waveforms.shape
                wvs = self.waveforms.copy()
                E = np.sqrt(np.nansum(self.waveforms ** 2, axis=2))
                zeroIdx = np.sum(E, 0) == [0, 0, 0, 0]
                E = E[:, ~zeroIdx]
                wvs = wvs[:, ~zeroIdx, :]
                normdWaves = (wvs.T / E.T).T
                PCA_m = self.getParam(normdWaves, &#39;PCA&#39;, fet=fet)
                # get mahalanobis distance
                idx = self.cut == cluster
                nClustSpikes = np.count_nonzero(idx)
                try:
                        d = self._mahal(PCA_m,PCA_m[idx,:])
                        # get the indices of the spikes not in the cluster
                        M_noise = d[~idx]
                        df = np.prod((fet, nElectrodes))
                        L = np.sum(1 - stats.chi2.cdf(M_noise, df))
                        L_ratio = L / nClustSpikes
                        # calculate isolation distance
                        if nClustSpikes &lt; nSpikes / 2:
                                M_noise.sort()
                                isolation_dist = M_noise[nClustSpikes]
                        else:
                                isolation_dist = np.nan
                except:
                        isolation_dist = L_ratio = np.nan
                return L_ratio, isolation_dist

        def _mahal(self, u, v):
                &#39;&#39;&#39;
                gets the mahalanobis distance between two vectors u and v
                a blatant copy of the Mathworks fcn as it doesn&#39;t require the covariance
                matrix to be calculated which is a pain if there are NaNs in the matrix
                &#39;&#39;&#39;
                u_sz = u.shape
                v_sz = v.shape
                if u_sz[1] != v_sz[1]:
                        warnings.warn(&#39;Input size mismatch: matrices must have same number of columns&#39;)
                if v_sz[0] &lt; v_sz[1]:
                        warnings.warn(&#39;Too few rows: v must have more rows than columns&#39;)
                if np.any(np.imag(u)) or np.any(np.imag(v)):
                        warnings.warn(&#39;No complex inputs are allowed&#39;)
                m = np.nanmean(v,axis=0)
                M = np.tile(m, reps=(u_sz[0],1))
                C = v - np.tile(m, reps=(v_sz[0],1))
                Q, R = np.linalg.qr(C)
                ri = np.linalg.solve(R.T, (u-M).T)
                d = np.sum(ri * ri,0).T * (v_sz[0]-1)
                return d

        def plotClusterSpace(self, clusters=None, param=&#39;Amp&#39;, clusts=None, bins=256, **kwargs):
                &#39;&#39;&#39;
                TODO: aspect of plot boxes in ImageGrid not right as scaled by range of
                values now
                &#39;&#39;&#39;
                import tintColours as tcols
                import matplotlib.colors as colors
                from itertools import combinations
                from mpl_toolkits.axes_grid1 import ImageGrid
                
                if isinstance(clusters, int):
                        clusters = [clusters]

                amps = self.getParam(param=param)
                bad_electrodes = np.setdiff1d(np.array(range(4)),np.array(np.sum(amps,0).nonzero())[0])
                cmap = np.tile(tcols.colours[0],(bins,1))
                cmap[0] = (1,1,1)
                cmap = colors.ListedColormap(cmap)
                cmap._init()
                alpha_vals = np.ones(cmap.N+3)
                alpha_vals[0] = 0
                cmap._lut[:,-1] = alpha_vals
                cmb = combinations(range(4),2)
                if &#39;fig&#39; in kwargs.keys():
                        fig = kwargs[&#39;fig&#39;]
                else:
                        fig = plt.figure(figsize=(8,6))
                if &#39;rect&#39; in kwargs.keys():
                        rect = kwargs[&#39;rect&#39;]
                else:
                        rect = 111
                grid = ImageGrid(fig, rect, nrows_ncols= (2,3), axes_pad=0.1, aspect=False)
                if &#39;Amp&#39; in param:
                        myRange = np.vstack((self.scaling*0, self.scaling*2))
                else:
                        myRange = None
                clustCMap0 = np.tile(tcols.colours[0],(bins,1))
                clustCMap0[0] = (1,1,1)
                clustCMap0 = colors.ListedColormap(clustCMap0)
                clustCMap0._init()
                clustCMap0._lut[:,-1] = alpha_vals
                for i, c in enumerate(cmb):
                        if c not in bad_electrodes:
                                h, ye, xe = np.histogram2d(amps[:,c[0]], amps[:,c[1]], range = myRange[:,c].T, bins=bins)
                                x, y = np.meshgrid(xe[0:-1], ye[0:-1])
                                grid[i].pcolormesh(x, y, h, cmap=clustCMap0, edgecolors=&#39;face&#39;)
                                if clusters is not None:
                                        for thisclust in clusters:
                                                clustidx = self.cut == thisclust
                                                h, ye, xe = np.histogram2d(amps[clustidx,c[0]],amps[clustidx,c[1]], range=myRange[:,c].T, bins=bins)
                                                clustCMap = np.tile(tcols.colours[thisclust],(bins,1))
                                                clustCMap[0] = (1,1,1)
                                                clustCMap = colors.ListedColormap(clustCMap)
                                                clustCMap._init()
                                                clustCMap._lut[:,-1] = alpha_vals
                                                grid[i].pcolormesh(x, y, h, cmap=clustCMap, edgecolors=&#39;face&#39;)
                        s = str(c[0]+1) + &#39; v &#39; + str(c[1]+1)
                        grid[i].text(0.05,0.95, s, va=&#39;top&#39;, ha=&#39;left&#39;, size=&#39;small&#39;, color=&#39;k&#39;, transform=grid[i].transAxes)
                        grid[i].set_xlim(xe.min(), xe.max())
                        grid[i].set_ylim(ye.min(), ye.max())
                plt.setp([a.get_xticklabels() for a in grid], visible=False)
                plt.setp([a.get_yticklabels() for a in grid], visible=False)
                return fig

        def p2t_time(self, cluster):
                &#34;&#34;&#34;
                The peak to trough time of a spike in ms
                
                Parameters
                ----------
                cluster: int
                        the cluster whose waveforms are to be analysed
                        
                Returns
                -------
                p2t: float
                        The mean peak-to-trough time for the channel (electrode) that has 
                        the strongest (highest amplitude) signal. Units are ms
                &#34;&#34;&#34;
                waveforms = self.waveforms[self.cut==cluster, :, :]
                best_chan = np.argmax(np.max(np.mean(waveforms, 0), 1))
                tP = self.getParam(waveforms, param=&#39;tP&#39;)
                tT = self.getParam(waveforms, param=&#39;tT&#39;)
                mn_tP = np.mean(tP, 0)
                mn_tT = np.mean(tT, 0)
                p2t = np.abs(mn_tP[best_chan] - mn_tT[best_chan])
                return p2t * 1000
                
        def half_amp_dur(self, cluster):
                &#34;&#34;&#34;
                Half amplitude duration of a spike
                
                Parameters
                ----------
                A: ndarray
                        An nSpikes x nElectrodes x nSamples array
                        
                Returns
                -------
                had: float
                        The half-amplitude duration for the channel (electrode) that has 
                        the strongest (highest amplitude) signal. Units are ms
                &#34;&#34;&#34;
                from scipy import interpolate, optimize
                
                waveforms = self.waveforms[self.cut==cluster, :, :]
                best_chan = np.argmax(np.max(np.mean(waveforms, 0), 1))
                mn_wvs = np.mean(waveforms, 0)
                wvs = mn_wvs[best_chan, :]
                half_amp = np.max(wvs) / 2
                half_amp = np.zeros_like(wvs) + half_amp
                t = np.linspace(0, 1/1000., 50)
                # create functions from the data using PiecewisePolynomial
                p1 = interpolate.PiecewisePolynomial(t, wvs[:,np.newaxis])
                p2 = interpolate.PiecewisePolynomial(t, half_amp[:,np.newaxis])
                xs = np.r_[t, t]
                xs.sort()
                x_min = xs.min()
                x_max = xs.max()
                x_mid = xs[:-1] + np.diff(xs) / 2
                roots = set()
                for val in x_mid:
                        root, infodict, ier, mesg = optimize.fsolve(lambda x: p1(x)-p2(x), val, full_output=True)
                        if ier==1 and x_min &lt; root &lt; x_max:
                                roots.add(root[0])
                roots = list(roots)
                if len(roots) &gt; 1:
                        r = np.abs(np.diff(roots[0:2]))[0]
                else:
                        r = np.nan
                return r

        def getParam(self, waveforms=None, param=&#39;Amp&#39;, t=200, fet=1):
                &#39;&#39;&#39;
                Returns the requested parameter from a spike train as a numpy array
                
                Parameters
                -------------------
                
                waveforms - numpy array         
                        Shape of array can be an nSpikes x nSamples
                        OR
                        a nSpikes x nElectrodes x nSamples
                
                param - str
                        Valid values are:
                                &#39;Amp&#39; - peak-to-trough amplitude (default)
                                &#39;P&#39; - height of peak
                                &#39;T&#39; - depth of trough
                                &#39;Vt&#39; height at time t
                                &#39;tP&#39; - time of peak (in seconds)
                                &#39;tT&#39; - time of trough (in seconds)
                                &#39;PCA&#39; - first n fet principal components (defaults to 1)
                                
                t - int
                        The time used for Vt
                        
                fet - int
                        The number of principal components (used with param &#39;PCA&#39;)
                &#39;&#39;&#39;
                from scipy import interpolate
                from sklearn.decomposition import PCA
                
                if waveforms is None:
                        waveforms = self.waveforms
                        
                if param == &#39;Amp&#39;:
                        return np.ptp(waveforms, axis=-1)
                elif param == &#39;P&#39;:
                        return np.max(waveforms, axis=-1)
                elif param == &#39;T&#39;:
                        return np.min(waveforms, axis=-1)
                elif param == &#39;Vt&#39;:
                        times = np.arange(0,1000,20)
                        f = interpolate.interp1d(times, range(50), &#39;nearest&#39;)
                        if waveforms.ndim == 2:
                                return waveforms[:, int(f(t))]
                        elif waveforms.ndim == 3:
                                return waveforms[:, :, int(f(t))]
                elif param == &#39;tP&#39;:
                        idx = np.argmax(waveforms, axis=-1)
                        m = interpolate.interp1d([0, waveforms.shape[-1]-1], [0, 1/1000.])
                        return m(idx)
                elif param == &#39;tT&#39;:
                        idx = np.argmin(waveforms, axis=-1)
                        m = interpolate.interp1d([0, waveforms.shape[-1]-1], [0, 1/1000.])
                        return m(idx)
                elif param == &#39;PCA&#39;:
                        pca = PCA(n_components=fet)
                        if waveforms.ndim == 2:
                                return pca.fit(waveforms).transform(waveforms).squeeze()
                        elif waveforms.ndim == 3:
                                out = np.zeros((waveforms.shape[0], waveforms.shape[1] * fet))
                                st = np.arange(0, waveforms.shape[1] * fet, fet)
                                en = np.arange(fet, fet + (waveforms.shape[1] * fet), fet)
                                rng = np.vstack((st, en))
                                for i in range(waveforms.shape[1]):
                                        if ~np.any(np.isnan(waveforms[:,i,:])):
                                                A = np.squeeze(pca.fit(waveforms[:,i,:].squeeze()).transform(waveforms[:,i,:].squeeze()))
                                                if A.ndim &lt; 2:
                                                        out[:,rng[0,i]:rng[1,i]] = np.atleast_2d(A).T
                                                else:
                                                        out[:,rng[0,i]:rng[1,i]] = A
                                return out</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ephysiopy.dacq2py.spikecalcs.SpikeCalcs"><code class="flex name class">
<span>class <span class="ident">SpikeCalcs</span></span>
</code></dt>
<dd>
<section class="desc"><p>Mix-in class for use with Tetrode class below.</p>
<p>Extends Tetrodes functionality by adding methods for analysis of spikes/
spike trains</p>
<p>Note lots of the methods here are native to dacq2py.axonaIO.Tetrode</p>
<p>Note that units are in milliseconds</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SpikeCalcs(object):
        &#34;&#34;&#34;
        Mix-in class for use with Tetrode class below.
        
        Extends Tetrodes functionality by adding methods for analysis of spikes/
        spike trains
                                
        Note lots of the methods here are native to dacq2py.axonaIO.Tetrode
        
        Note that units are in milliseconds
        &#34;&#34;&#34;
        
        
        def getNSpikes(self, cluster):
                if cluster not in self.clusters:
                        warnings.warn(&#39;Cluster not available. Try again!&#39;)
                else:
                        return np.count_nonzero(self.cut == cluster)

        def trial_av_firing_rate(self, cluster):
                &#39;&#39;&#39;
                returns the trial average firing rate of a cluster in Hz
                &#39;&#39;&#39;
                return self.getNSpikes(cluster) / float(self.header[&#39;duration&#39;])
        
        def mean_autoCorr(self, cluster, n=40):
                &#39;&#39;&#39;
                Returns the autocorrelation function mean from 0 to n ms (default=40)
                Used to help classify units as principal or interneuron
                &#39;&#39;&#39;
                if cluster not in self.clusters:
                        warnings.warn(&#39;Cluster not available. Try again!&#39;)
                else:
                        bins = 201
                        Trange = (-500, 500)
                        y = self.xcorr(cluster, Trange=Trange)
                        counts, bins = np.histogram(y[y != 0], bins=bins, range=Trange)
                        mask = np.logical_and(bins&gt;0, bins&lt;n)
                        return np.mean(counts[mask])
                
        def ifr_sp_corr(self, clusterA, speed, minSpeed=2.0, maxSpeed=40.0, sigma=3, 
                                        shuffle=False, nShuffles=100, minTime=30, plot=False):
                &#34;&#34;&#34;
                clusterA: int
                        the cluster to do the correlation with speed
                speed: np.array (1 x nSamples)
                        instantaneous speed 
                minSpeed: int
                        speeds below this value are ignored - defaults to 2cm/s as with
                        Kropff et al., 2015
                &#34;&#34;&#34;
                if clusterA not in self.clusters:
                        warnings.warn(&#39;Cluster not available. Try again!&#39;)
                else:
                        speed = speed.ravel()
                        posSampRate = 50
                        nSamples = len(speed)
                        x1 = self.getClustIdx(clusterA)
                        # position is sampled at 50Hz and so is &#39;automatically&#39; binned into
                        # 20ms bins
                        spk_hist = np.bincount(x1, minlength=nSamples)
                        # smooth the spk_hist (which is a temporal histogram) with a 250ms
                        # gaussian as with Kropff et al., 2015
                        h = signal.gaussian(13, sigma)
                        h = h / float(np.sum(h))
                        #filter for low speeds
                        lowSpeedIdx = speed &lt; minSpeed
                        highSpeedIdx = speed &gt; maxSpeed
                        speed_filt = speed[~np.logical_or(lowSpeedIdx, highSpeedIdx)]
                        spk_hist_filt = spk_hist[~np.logical_or(lowSpeedIdx, highSpeedIdx)]
                        spk_sm = signal.filtfilt(h.ravel(), 1, spk_hist_filt)
                        sm_spk_rate = spk_sm * posSampRate
                        
                        res = stats.pearsonr(sm_spk_rate, speed_filt)
                        if plot:            
                                # do some fancy plotting stuff
                                speed_binned, sp_bin_edges = np.histogram(speed_filt, bins=50)
                                sp_dig = np.digitize(speed_filt, sp_bin_edges, right=True)
                                spks_per_sp_bin = [spk_hist_filt[sp_dig==i] for i in range(len(sp_bin_edges))] 
                                rate_per_sp_bin = []
                                for x in spks_per_sp_bin:
                                        rate_per_sp_bin.append(np.mean(x) * posSampRate)
                                rate_filter = signal.gaussian(5, 1.0)
                                rate_filter = rate_filter / np.sum(rate_filter)
                                binned_spk_rate = signal.filtfilt(rate_filter, 1, rate_per_sp_bin)
                                # instead of plotting a scatter plot of the firing rate at each 
                                # speed bin, plot a log normalised heatmap and overlay results on it
                                
                                spk_binning_edges = np.linspace(np.min(sm_spk_rate), np.max(sm_spk_rate),
                                                                                                len(sp_bin_edges))
                                speed_mesh, spk_mesh = np.meshgrid(sp_bin_edges, spk_binning_edges)
                                binned_rate, _, _ = np.histogram2d(speed_filt, sm_spk_rate, bins=[sp_bin_edges,
                                                                                                   spk_binning_edges])
                                #blur the binned rate a bit to make it look nicer
                                sm_binned_rate = blur_image(binned_rate, 5)
                                plt.figure()
                                plt.pcolormesh(speed_mesh, spk_mesh, sm_binned_rate, norm=colors.LogNorm(), alpha=0.5, shading=&#39;flat&#39;, edgecolors=&#39;None&#39;)
                                #overlay the smoothed binned rate against speed
                                plt.hold(True)
                                plt.plot(sp_bin_edges, binned_spk_rate, &#39;r&#39;)
                                #do the linear regression and plot the fit too
                                # TODO: linear regression is broken ie not regressing the correct variables
                                lr = stats.linregress(speed_filt, sm_spk_rate)
                                end_point = lr.intercept + ((sp_bin_edges[-1] - sp_bin_edges[0]) * lr.slope)
                                plt.plot([np.min(sp_bin_edges), np.max(sp_bin_edges)], [lr.intercept, end_point], &#39;r--&#39;)
                                ax = plt.gca()
                                ax.set_xlim(np.min(sp_bin_edges), np.max(sp_bin_edges[-2]))
                                ax.set_ylim(0, np.nanmax(binned_spk_rate) * 1.1)
                                ax.set_ylabel(&#39;Firing rate(Hz)&#39;)
                                ax.set_xlabel(&#39;Running speed(cm/s)&#39;)
                                ax.set_title(&#39;Intercept: {0:.3f}    Slope: {1:.5f}\nPearson: {2:.5f}&#39;.format(lr.intercept, lr.slope, lr.rvalue))
                        #do some shuffling of the data to see if the result is signficant            
                        if shuffle:                
                                # shift spikes by at least 30 seconds after trial start and
                                # 30 seconds before trial end
                                timeSteps = np.random.randint(30 * posSampRate, nSamples - (30 * posSampRate),
                                                                                                          nShuffles)
                                shuffled_results = []            
                                for t in timeSteps:
                                        spk_count = np.roll(spk_hist, t)
                                        spk_count_filt = spk_count[~lowSpeedIdx]
                                        spk_count_sm = signal.filtfilt(h.ravel(), 1, spk_count_filt)
                                        shuffled_results.append(stats.pearsonr(spk_count_sm, speed_filt)[0])
                                if plot:
                                        plt.figure()
                                        ax = plt.gca()
                                        ax.hist(np.abs(shuffled_results), 20)
                                        ylims = ax.get_ylim()
                                        ax.vlines(res, ylims[0], ylims[1], &#39;r&#39;)
                                
                        print(&#34;PPMC: {0}&#34;.format(res[0]))

        def xcorr(self, x1, x2=None, Trange=None):
                &#39;&#39;&#39;
                Returns the histogram of the ISIs

                Parameters
                ---------------
                x1 - 1d np.array list of spike times
                x2 - (optional) 1d np.array of spike times
                Trange - 1x2 np.array for range of times to bin up. Defaults
                                        to [-500, +500]
                &#39;&#39;&#39;
                if x2 is None:
                        x2 = x1.copy()
                if Trange is None:
                        Trange = np.array([-500, 500])
                y = []
                irange = x1[:, np.newaxis] + Trange[np.newaxis, :]
                dts = np.searchsorted(x2, irange)
                for i, t in enumerate(dts):
                        y.extend(x2[t[0]:t[1]] - x1[i])
                y = np.array(y, dtype=float)
                return y

        def smoothSpikePosCount(self, x1, npos, sigma=3.0, shuffle=None):
                &#39;&#39;&#39;
                Returns a spike train the same length as num pos samples that has been
                smoothed in time with a gaussian kernel M in width and standard deviation
                equal to sigma
                
                Parameters
                --------------
                x1 : np.array
                        The pos indices the spikes occured at
                npos : int
                        The number of position samples captured
                sigma : float
                        the standard deviation of the gaussian used to smooth the spike
                        train
                shuffle: int
                        The number of seconds to shift the spike train by. Default None
                
                Returns
                -----------
                smoothed_spikes : np.array
                        The smoothed spike train
                &#39;&#39;&#39;
                spk_hist = np.bincount(x1, minlength=npos)
                if shuffle is not None:
                        spk_hist = np.roll(spk_hist, int(shuffle * 50))
                # smooth the spk_hist (which is a temporal histogram) with a 250ms
                # gaussian as with Kropff et al., 2015
                h = signal.gaussian(13, sigma)
                h = h / float(np.sum(h))
                return signal.filtfilt(h.ravel(), 1, spk_hist)
        
        def getMeanWaveform(self, clusterA):
                &#39;&#39;&#39;
                Returns the mean waveform and sem for a given spike train
                
                Parameters
                ----------
                clusterA: int
                        The cluster to get the mean waveform for
                        
                Returns
                -------
                mn_wvs: ndarray (floats) - usually 4x50 for tetrode recordings
                        the mean waveforms
                std_wvs: ndarray (floats) - usually 4x50 for tetrode recordings
                        the standard deviations of the waveforms
                &#39;&#39;&#39;
                if clusterA not in self.clusters:
                        warnings.warn(&#39;Cluster not available. Try again!&#39;)
                x = self.getClustSpks(clusterA)
                return np.mean(x, axis=0), np.std(x, axis=0)

        def thetaBandMaxFreq(self, x1):
                &#39;&#39;&#39;
                Calculates the frequency with the max power in the theta band (6-12Hz)
                of a spike trains autocorrelogram. Partly to look for differences
                in theta frequency in different running directions a la Blair (Welday paper)
                &#39;&#39;&#39;
                y = self.xcorr(x1)
                corr, _ = np.histogram(y[y != 0], bins=201, range=np.array([-500,500]))
                # Take the fft of the spike train autocorr (from -500 to +500ms)
                from scipy.signal import periodogram
                freqs, power = periodogram(corr, fs=200, return_onesided=True)
                power_masked = np.ma.MaskedArray(power,np.logical_or(freqs&lt;6,freqs&gt;12))
                return freqs[np.argmax(power_masked)]

        def thetaModIdx(self, x1):
                &#39;&#39;&#39;
                Calculates a theta modulation index of a spike train based on the cells
                autocorrelogram
                
                Parameters
                ----------
                x1: np.array
                        The spike time-series
                Returns
                -------
                thetaMod: float
                        The difference of the values at the first peak and trough of the
                        autocorrelogram
                &#39;&#39;&#39;
                y = self.xcorr(x1)
                corr, _ = np.histogram(y[y != 0], bins=201, range=np.array([-500,500]))
                # Take the fft of the spike train autocorr (from -500 to +500ms)
                from scipy.signal import periodogram
                freqs, power = periodogram(corr, fs=200, return_onesided=True)
                # Smooth the power over +/- 1Hz
                b = signal.boxcar(3)
                h = signal.filtfilt(b, 3, power)
                
                # Square the amplitude first
                sqd_amp = h ** 2
                # Then find the mean power in the +/-1Hz band either side of that
                theta_band_max_idx = np.nonzero(sqd_amp==np.max(sqd_amp[np.logical_and(freqs&gt;6, freqs&lt;11)]))[0][0]
                mean_theta_band_power = np.mean(sqd_amp[theta_band_max_idx-1:theta_band_max_idx+1])
                # Find the mean amplitude in the 2-50Hz range
                other_band_idx = np.logical_and(freqs&gt;2, freqs&lt;50)
                mean_other_band_power = np.mean(sqd_amp[other_band_idx])
                # Find the ratio of these two - this is the theta modulation index
                return (mean_theta_band_power - mean_other_band_power) / (mean_theta_band_power + mean_other_band_power)

        def thetaModIdxV2(self, x1):
                &#39;&#39;&#39;
                This is a simpler alternative to the thetaModIdx method in that it
                calculates the difference between the normalized temporal autocorrelogram
                at the trough between 50-70ms and the peak between 100-140ms over
                their sum (data is binned into 5ms bins)
                
                Measure used in Cacucci et al., 2004 and Kropff et al 2015
                &#39;&#39;&#39;
                y = self.xcorr(x1)
                corr, bins = np.histogram(y[y != 0], bins=201, range=np.array([-500,500]))
                # &#39;close&#39; the right-hand bin
                bins = bins[0:-1]
                # normalise corr so max is 1.0
                corr = corr/float(np.max(corr))
                thetaAntiPhase = np.min(corr[np.logical_and(bins&gt;50,bins&lt;70)])
                thetaPhase = np.max(corr[np.logical_and(bins&gt;100, bins&lt;140)])
                return (thetaPhase-thetaAntiPhase) / (thetaPhase+thetaAntiPhase)

        def clusterQuality(self, cluster, fet=1):
                &#39;&#39;&#39;
                returns the L-ratio and Isolation Distance measures
                calculated on the principal components of the energy in a spike matrix
                &#39;&#39;&#39;
                nSpikes, nElectrodes, _ = self.waveforms.shape
                wvs = self.waveforms.copy()
                E = np.sqrt(np.nansum(self.waveforms ** 2, axis=2))
                zeroIdx = np.sum(E, 0) == [0, 0, 0, 0]
                E = E[:, ~zeroIdx]
                wvs = wvs[:, ~zeroIdx, :]
                normdWaves = (wvs.T / E.T).T
                PCA_m = self.getParam(normdWaves, &#39;PCA&#39;, fet=fet)
                # get mahalanobis distance
                idx = self.cut == cluster
                nClustSpikes = np.count_nonzero(idx)
                try:
                        d = self._mahal(PCA_m,PCA_m[idx,:])
                        # get the indices of the spikes not in the cluster
                        M_noise = d[~idx]
                        df = np.prod((fet, nElectrodes))
                        L = np.sum(1 - stats.chi2.cdf(M_noise, df))
                        L_ratio = L / nClustSpikes
                        # calculate isolation distance
                        if nClustSpikes &lt; nSpikes / 2:
                                M_noise.sort()
                                isolation_dist = M_noise[nClustSpikes]
                        else:
                                isolation_dist = np.nan
                except:
                        isolation_dist = L_ratio = np.nan
                return L_ratio, isolation_dist

        def _mahal(self, u, v):
                &#39;&#39;&#39;
                gets the mahalanobis distance between two vectors u and v
                a blatant copy of the Mathworks fcn as it doesn&#39;t require the covariance
                matrix to be calculated which is a pain if there are NaNs in the matrix
                &#39;&#39;&#39;
                u_sz = u.shape
                v_sz = v.shape
                if u_sz[1] != v_sz[1]:
                        warnings.warn(&#39;Input size mismatch: matrices must have same number of columns&#39;)
                if v_sz[0] &lt; v_sz[1]:
                        warnings.warn(&#39;Too few rows: v must have more rows than columns&#39;)
                if np.any(np.imag(u)) or np.any(np.imag(v)):
                        warnings.warn(&#39;No complex inputs are allowed&#39;)
                m = np.nanmean(v,axis=0)
                M = np.tile(m, reps=(u_sz[0],1))
                C = v - np.tile(m, reps=(v_sz[0],1))
                Q, R = np.linalg.qr(C)
                ri = np.linalg.solve(R.T, (u-M).T)
                d = np.sum(ri * ri,0).T * (v_sz[0]-1)
                return d

        def plotClusterSpace(self, clusters=None, param=&#39;Amp&#39;, clusts=None, bins=256, **kwargs):
                &#39;&#39;&#39;
                TODO: aspect of plot boxes in ImageGrid not right as scaled by range of
                values now
                &#39;&#39;&#39;
                import tintColours as tcols
                import matplotlib.colors as colors
                from itertools import combinations
                from mpl_toolkits.axes_grid1 import ImageGrid
                
                if isinstance(clusters, int):
                        clusters = [clusters]

                amps = self.getParam(param=param)
                bad_electrodes = np.setdiff1d(np.array(range(4)),np.array(np.sum(amps,0).nonzero())[0])
                cmap = np.tile(tcols.colours[0],(bins,1))
                cmap[0] = (1,1,1)
                cmap = colors.ListedColormap(cmap)
                cmap._init()
                alpha_vals = np.ones(cmap.N+3)
                alpha_vals[0] = 0
                cmap._lut[:,-1] = alpha_vals
                cmb = combinations(range(4),2)
                if &#39;fig&#39; in kwargs.keys():
                        fig = kwargs[&#39;fig&#39;]
                else:
                        fig = plt.figure(figsize=(8,6))
                if &#39;rect&#39; in kwargs.keys():
                        rect = kwargs[&#39;rect&#39;]
                else:
                        rect = 111
                grid = ImageGrid(fig, rect, nrows_ncols= (2,3), axes_pad=0.1, aspect=False)
                if &#39;Amp&#39; in param:
                        myRange = np.vstack((self.scaling*0, self.scaling*2))
                else:
                        myRange = None
                clustCMap0 = np.tile(tcols.colours[0],(bins,1))
                clustCMap0[0] = (1,1,1)
                clustCMap0 = colors.ListedColormap(clustCMap0)
                clustCMap0._init()
                clustCMap0._lut[:,-1] = alpha_vals
                for i, c in enumerate(cmb):
                        if c not in bad_electrodes:
                                h, ye, xe = np.histogram2d(amps[:,c[0]], amps[:,c[1]], range = myRange[:,c].T, bins=bins)
                                x, y = np.meshgrid(xe[0:-1], ye[0:-1])
                                grid[i].pcolormesh(x, y, h, cmap=clustCMap0, edgecolors=&#39;face&#39;)
                                if clusters is not None:
                                        for thisclust in clusters:
                                                clustidx = self.cut == thisclust
                                                h, ye, xe = np.histogram2d(amps[clustidx,c[0]],amps[clustidx,c[1]], range=myRange[:,c].T, bins=bins)
                                                clustCMap = np.tile(tcols.colours[thisclust],(bins,1))
                                                clustCMap[0] = (1,1,1)
                                                clustCMap = colors.ListedColormap(clustCMap)
                                                clustCMap._init()
                                                clustCMap._lut[:,-1] = alpha_vals
                                                grid[i].pcolormesh(x, y, h, cmap=clustCMap, edgecolors=&#39;face&#39;)
                        s = str(c[0]+1) + &#39; v &#39; + str(c[1]+1)
                        grid[i].text(0.05,0.95, s, va=&#39;top&#39;, ha=&#39;left&#39;, size=&#39;small&#39;, color=&#39;k&#39;, transform=grid[i].transAxes)
                        grid[i].set_xlim(xe.min(), xe.max())
                        grid[i].set_ylim(ye.min(), ye.max())
                plt.setp([a.get_xticklabels() for a in grid], visible=False)
                plt.setp([a.get_yticklabels() for a in grid], visible=False)
                return fig

        def p2t_time(self, cluster):
                &#34;&#34;&#34;
                The peak to trough time of a spike in ms
                
                Parameters
                ----------
                cluster: int
                        the cluster whose waveforms are to be analysed
                        
                Returns
                -------
                p2t: float
                        The mean peak-to-trough time for the channel (electrode) that has 
                        the strongest (highest amplitude) signal. Units are ms
                &#34;&#34;&#34;
                waveforms = self.waveforms[self.cut==cluster, :, :]
                best_chan = np.argmax(np.max(np.mean(waveforms, 0), 1))
                tP = self.getParam(waveforms, param=&#39;tP&#39;)
                tT = self.getParam(waveforms, param=&#39;tT&#39;)
                mn_tP = np.mean(tP, 0)
                mn_tT = np.mean(tT, 0)
                p2t = np.abs(mn_tP[best_chan] - mn_tT[best_chan])
                return p2t * 1000
                
        def half_amp_dur(self, cluster):
                &#34;&#34;&#34;
                Half amplitude duration of a spike
                
                Parameters
                ----------
                A: ndarray
                        An nSpikes x nElectrodes x nSamples array
                        
                Returns
                -------
                had: float
                        The half-amplitude duration for the channel (electrode) that has 
                        the strongest (highest amplitude) signal. Units are ms
                &#34;&#34;&#34;
                from scipy import interpolate, optimize
                
                waveforms = self.waveforms[self.cut==cluster, :, :]
                best_chan = np.argmax(np.max(np.mean(waveforms, 0), 1))
                mn_wvs = np.mean(waveforms, 0)
                wvs = mn_wvs[best_chan, :]
                half_amp = np.max(wvs) / 2
                half_amp = np.zeros_like(wvs) + half_amp
                t = np.linspace(0, 1/1000., 50)
                # create functions from the data using PiecewisePolynomial
                p1 = interpolate.PiecewisePolynomial(t, wvs[:,np.newaxis])
                p2 = interpolate.PiecewisePolynomial(t, half_amp[:,np.newaxis])
                xs = np.r_[t, t]
                xs.sort()
                x_min = xs.min()
                x_max = xs.max()
                x_mid = xs[:-1] + np.diff(xs) / 2
                roots = set()
                for val in x_mid:
                        root, infodict, ier, mesg = optimize.fsolve(lambda x: p1(x)-p2(x), val, full_output=True)
                        if ier==1 and x_min &lt; root &lt; x_max:
                                roots.add(root[0])
                roots = list(roots)
                if len(roots) &gt; 1:
                        r = np.abs(np.diff(roots[0:2]))[0]
                else:
                        r = np.nan
                return r

        def getParam(self, waveforms=None, param=&#39;Amp&#39;, t=200, fet=1):
                &#39;&#39;&#39;
                Returns the requested parameter from a spike train as a numpy array
                
                Parameters
                -------------------
                
                waveforms - numpy array         
                        Shape of array can be an nSpikes x nSamples
                        OR
                        a nSpikes x nElectrodes x nSamples
                
                param - str
                        Valid values are:
                                &#39;Amp&#39; - peak-to-trough amplitude (default)
                                &#39;P&#39; - height of peak
                                &#39;T&#39; - depth of trough
                                &#39;Vt&#39; height at time t
                                &#39;tP&#39; - time of peak (in seconds)
                                &#39;tT&#39; - time of trough (in seconds)
                                &#39;PCA&#39; - first n fet principal components (defaults to 1)
                                
                t - int
                        The time used for Vt
                        
                fet - int
                        The number of principal components (used with param &#39;PCA&#39;)
                &#39;&#39;&#39;
                from scipy import interpolate
                from sklearn.decomposition import PCA
                
                if waveforms is None:
                        waveforms = self.waveforms
                        
                if param == &#39;Amp&#39;:
                        return np.ptp(waveforms, axis=-1)
                elif param == &#39;P&#39;:
                        return np.max(waveforms, axis=-1)
                elif param == &#39;T&#39;:
                        return np.min(waveforms, axis=-1)
                elif param == &#39;Vt&#39;:
                        times = np.arange(0,1000,20)
                        f = interpolate.interp1d(times, range(50), &#39;nearest&#39;)
                        if waveforms.ndim == 2:
                                return waveforms[:, int(f(t))]
                        elif waveforms.ndim == 3:
                                return waveforms[:, :, int(f(t))]
                elif param == &#39;tP&#39;:
                        idx = np.argmax(waveforms, axis=-1)
                        m = interpolate.interp1d([0, waveforms.shape[-1]-1], [0, 1/1000.])
                        return m(idx)
                elif param == &#39;tT&#39;:
                        idx = np.argmin(waveforms, axis=-1)
                        m = interpolate.interp1d([0, waveforms.shape[-1]-1], [0, 1/1000.])
                        return m(idx)
                elif param == &#39;PCA&#39;:
                        pca = PCA(n_components=fet)
                        if waveforms.ndim == 2:
                                return pca.fit(waveforms).transform(waveforms).squeeze()
                        elif waveforms.ndim == 3:
                                out = np.zeros((waveforms.shape[0], waveforms.shape[1] * fet))
                                st = np.arange(0, waveforms.shape[1] * fet, fet)
                                en = np.arange(fet, fet + (waveforms.shape[1] * fet), fet)
                                rng = np.vstack((st, en))
                                for i in range(waveforms.shape[1]):
                                        if ~np.any(np.isnan(waveforms[:,i,:])):
                                                A = np.squeeze(pca.fit(waveforms[:,i,:].squeeze()).transform(waveforms[:,i,:].squeeze()))
                                                if A.ndim &lt; 2:
                                                        out[:,rng[0,i]:rng[1,i]] = np.atleast_2d(A).T
                                                else:
                                                        out[:,rng[0,i]:rng[1,i]] = A
                                return out</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="ephysiopy.dacq2py.axonaIO.Tetrode" href="axonaIO.html#ephysiopy.dacq2py.axonaIO.Tetrode">Tetrode</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.clusterQuality"><code class="name flex">
<span>def <span class="ident">clusterQuality</span></span>(<span>self, cluster, fet=1)</span>
</code></dt>
<dd>
<section class="desc"><p>returns the L-ratio and Isolation Distance measures
calculated on the principal components of the energy in a spike matrix</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clusterQuality(self, cluster, fet=1):
        &#39;&#39;&#39;
        returns the L-ratio and Isolation Distance measures
        calculated on the principal components of the energy in a spike matrix
        &#39;&#39;&#39;
        nSpikes, nElectrodes, _ = self.waveforms.shape
        wvs = self.waveforms.copy()
        E = np.sqrt(np.nansum(self.waveforms ** 2, axis=2))
        zeroIdx = np.sum(E, 0) == [0, 0, 0, 0]
        E = E[:, ~zeroIdx]
        wvs = wvs[:, ~zeroIdx, :]
        normdWaves = (wvs.T / E.T).T
        PCA_m = self.getParam(normdWaves, &#39;PCA&#39;, fet=fet)
        # get mahalanobis distance
        idx = self.cut == cluster
        nClustSpikes = np.count_nonzero(idx)
        try:
                d = self._mahal(PCA_m,PCA_m[idx,:])
                # get the indices of the spikes not in the cluster
                M_noise = d[~idx]
                df = np.prod((fet, nElectrodes))
                L = np.sum(1 - stats.chi2.cdf(M_noise, df))
                L_ratio = L / nClustSpikes
                # calculate isolation distance
                if nClustSpikes &lt; nSpikes / 2:
                        M_noise.sort()
                        isolation_dist = M_noise[nClustSpikes]
                else:
                        isolation_dist = np.nan
        except:
                isolation_dist = L_ratio = np.nan
        return L_ratio, isolation_dist</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.getMeanWaveform"><code class="name flex">
<span>def <span class="ident">getMeanWaveform</span></span>(<span>self, clusterA)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the mean waveform and sem for a given spike train</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>clusterA</code></strong> :&ensp;<code>int</code></dt>
<dd>The cluster to get the mean waveform for</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mn_wvs</code></strong> :&ensp;<code>ndarray</code> (<code>floats</code>) - <code>usually</code> <code>4x50</code> <code>for</code> <code>tetrode</code> <code>recordings</code></dt>
<dd>the mean waveforms</dd>
<dt><strong><code>std_wvs</code></strong> :&ensp;<code>ndarray</code> (<code>floats</code>) - <code>usually</code> <code>4x50</code> <code>for</code> <code>tetrode</code> <code>recordings</code></dt>
<dd>the standard deviations of the waveforms</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getMeanWaveform(self, clusterA):
        &#39;&#39;&#39;
        Returns the mean waveform and sem for a given spike train
        
        Parameters
        ----------
        clusterA: int
                The cluster to get the mean waveform for
                
        Returns
        -------
        mn_wvs: ndarray (floats) - usually 4x50 for tetrode recordings
                the mean waveforms
        std_wvs: ndarray (floats) - usually 4x50 for tetrode recordings
                the standard deviations of the waveforms
        &#39;&#39;&#39;
        if clusterA not in self.clusters:
                warnings.warn(&#39;Cluster not available. Try again!&#39;)
        x = self.getClustSpks(clusterA)
        return np.mean(x, axis=0), np.std(x, axis=0)</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.getNSpikes"><code class="name flex">
<span>def <span class="ident">getNSpikes</span></span>(<span>self, cluster)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getNSpikes(self, cluster):
        if cluster not in self.clusters:
                warnings.warn(&#39;Cluster not available. Try again!&#39;)
        else:
                return np.count_nonzero(self.cut == cluster)</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.getParam"><code class="name flex">
<span>def <span class="ident">getParam</span></span>(<span>self, waveforms=None, param='Amp', t=200, fet=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the requested parameter from a spike train as a numpy array</p>
<h2 id="parameters">Parameters</h2>
<p>waveforms - numpy array
<br>
Shape of array can be an nSpikes x nSamples
OR
a nSpikes x nElectrodes x nSamples</p>
<p>param - str
Valid values are:
'Amp' - peak-to-trough amplitude (default)
'P' - height of peak
'T' - depth of trough
'Vt' height at time t
'tP' - time of peak (in seconds)
'tT' - time of trough (in seconds)
'PCA' - first n fet principal components (defaults to 1)</p>
<p>t - int
The time used for Vt</p>
<p>fet - int
The number of principal components (used with param 'PCA')</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getParam(self, waveforms=None, param=&#39;Amp&#39;, t=200, fet=1):
        &#39;&#39;&#39;
        Returns the requested parameter from a spike train as a numpy array
        
        Parameters
        -------------------
        
        waveforms - numpy array         
                Shape of array can be an nSpikes x nSamples
                OR
                a nSpikes x nElectrodes x nSamples
        
        param - str
                Valid values are:
                        &#39;Amp&#39; - peak-to-trough amplitude (default)
                        &#39;P&#39; - height of peak
                        &#39;T&#39; - depth of trough
                        &#39;Vt&#39; height at time t
                        &#39;tP&#39; - time of peak (in seconds)
                        &#39;tT&#39; - time of trough (in seconds)
                        &#39;PCA&#39; - first n fet principal components (defaults to 1)
                        
        t - int
                The time used for Vt
                
        fet - int
                The number of principal components (used with param &#39;PCA&#39;)
        &#39;&#39;&#39;
        from scipy import interpolate
        from sklearn.decomposition import PCA
        
        if waveforms is None:
                waveforms = self.waveforms
                
        if param == &#39;Amp&#39;:
                return np.ptp(waveforms, axis=-1)
        elif param == &#39;P&#39;:
                return np.max(waveforms, axis=-1)
        elif param == &#39;T&#39;:
                return np.min(waveforms, axis=-1)
        elif param == &#39;Vt&#39;:
                times = np.arange(0,1000,20)
                f = interpolate.interp1d(times, range(50), &#39;nearest&#39;)
                if waveforms.ndim == 2:
                        return waveforms[:, int(f(t))]
                elif waveforms.ndim == 3:
                        return waveforms[:, :, int(f(t))]
        elif param == &#39;tP&#39;:
                idx = np.argmax(waveforms, axis=-1)
                m = interpolate.interp1d([0, waveforms.shape[-1]-1], [0, 1/1000.])
                return m(idx)
        elif param == &#39;tT&#39;:
                idx = np.argmin(waveforms, axis=-1)
                m = interpolate.interp1d([0, waveforms.shape[-1]-1], [0, 1/1000.])
                return m(idx)
        elif param == &#39;PCA&#39;:
                pca = PCA(n_components=fet)
                if waveforms.ndim == 2:
                        return pca.fit(waveforms).transform(waveforms).squeeze()
                elif waveforms.ndim == 3:
                        out = np.zeros((waveforms.shape[0], waveforms.shape[1] * fet))
                        st = np.arange(0, waveforms.shape[1] * fet, fet)
                        en = np.arange(fet, fet + (waveforms.shape[1] * fet), fet)
                        rng = np.vstack((st, en))
                        for i in range(waveforms.shape[1]):
                                if ~np.any(np.isnan(waveforms[:,i,:])):
                                        A = np.squeeze(pca.fit(waveforms[:,i,:].squeeze()).transform(waveforms[:,i,:].squeeze()))
                                        if A.ndim &lt; 2:
                                                out[:,rng[0,i]:rng[1,i]] = np.atleast_2d(A).T
                                        else:
                                                out[:,rng[0,i]:rng[1,i]] = A
                        return out</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.half_amp_dur"><code class="name flex">
<span>def <span class="ident">half_amp_dur</span></span>(<span>self, cluster)</span>
</code></dt>
<dd>
<section class="desc"><p>Half amplitude duration of a spike</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>A</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>An nSpikes x nElectrodes x nSamples array</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>had</code></strong> :&ensp;<code>float</code></dt>
<dd>The half-amplitude duration for the channel (electrode) that has
the strongest (highest amplitude) signal. Units are ms</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def half_amp_dur(self, cluster):
        &#34;&#34;&#34;
        Half amplitude duration of a spike
        
        Parameters
        ----------
        A: ndarray
                An nSpikes x nElectrodes x nSamples array
                
        Returns
        -------
        had: float
                The half-amplitude duration for the channel (electrode) that has 
                the strongest (highest amplitude) signal. Units are ms
        &#34;&#34;&#34;
        from scipy import interpolate, optimize
        
        waveforms = self.waveforms[self.cut==cluster, :, :]
        best_chan = np.argmax(np.max(np.mean(waveforms, 0), 1))
        mn_wvs = np.mean(waveforms, 0)
        wvs = mn_wvs[best_chan, :]
        half_amp = np.max(wvs) / 2
        half_amp = np.zeros_like(wvs) + half_amp
        t = np.linspace(0, 1/1000., 50)
        # create functions from the data using PiecewisePolynomial
        p1 = interpolate.PiecewisePolynomial(t, wvs[:,np.newaxis])
        p2 = interpolate.PiecewisePolynomial(t, half_amp[:,np.newaxis])
        xs = np.r_[t, t]
        xs.sort()
        x_min = xs.min()
        x_max = xs.max()
        x_mid = xs[:-1] + np.diff(xs) / 2
        roots = set()
        for val in x_mid:
                root, infodict, ier, mesg = optimize.fsolve(lambda x: p1(x)-p2(x), val, full_output=True)
                if ier==1 and x_min &lt; root &lt; x_max:
                        roots.add(root[0])
        roots = list(roots)
        if len(roots) &gt; 1:
                r = np.abs(np.diff(roots[0:2]))[0]
        else:
                r = np.nan
        return r</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.ifr_sp_corr"><code class="name flex">
<span>def <span class="ident">ifr_sp_corr</span></span>(<span>self, clusterA, speed, minSpeed=2.0, maxSpeed=40.0, sigma=3, shuffle=False, nShuffles=100, minTime=30, plot=False)</span>
</code></dt>
<dd>
<section class="desc"><p>clusterA: int
the cluster to do the correlation with speed
speed: np.array (1 x nSamples)
instantaneous speed
minSpeed: int
speeds below this value are ignored - defaults to 2cm/s as with
Kropff et al., 2015</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ifr_sp_corr(self, clusterA, speed, minSpeed=2.0, maxSpeed=40.0, sigma=3, 
                                shuffle=False, nShuffles=100, minTime=30, plot=False):
        &#34;&#34;&#34;
        clusterA: int
                the cluster to do the correlation with speed
        speed: np.array (1 x nSamples)
                instantaneous speed 
        minSpeed: int
                speeds below this value are ignored - defaults to 2cm/s as with
                Kropff et al., 2015
        &#34;&#34;&#34;
        if clusterA not in self.clusters:
                warnings.warn(&#39;Cluster not available. Try again!&#39;)
        else:
                speed = speed.ravel()
                posSampRate = 50
                nSamples = len(speed)
                x1 = self.getClustIdx(clusterA)
                # position is sampled at 50Hz and so is &#39;automatically&#39; binned into
                # 20ms bins
                spk_hist = np.bincount(x1, minlength=nSamples)
                # smooth the spk_hist (which is a temporal histogram) with a 250ms
                # gaussian as with Kropff et al., 2015
                h = signal.gaussian(13, sigma)
                h = h / float(np.sum(h))
                #filter for low speeds
                lowSpeedIdx = speed &lt; minSpeed
                highSpeedIdx = speed &gt; maxSpeed
                speed_filt = speed[~np.logical_or(lowSpeedIdx, highSpeedIdx)]
                spk_hist_filt = spk_hist[~np.logical_or(lowSpeedIdx, highSpeedIdx)]
                spk_sm = signal.filtfilt(h.ravel(), 1, spk_hist_filt)
                sm_spk_rate = spk_sm * posSampRate
                
                res = stats.pearsonr(sm_spk_rate, speed_filt)
                if plot:            
                        # do some fancy plotting stuff
                        speed_binned, sp_bin_edges = np.histogram(speed_filt, bins=50)
                        sp_dig = np.digitize(speed_filt, sp_bin_edges, right=True)
                        spks_per_sp_bin = [spk_hist_filt[sp_dig==i] for i in range(len(sp_bin_edges))] 
                        rate_per_sp_bin = []
                        for x in spks_per_sp_bin:
                                rate_per_sp_bin.append(np.mean(x) * posSampRate)
                        rate_filter = signal.gaussian(5, 1.0)
                        rate_filter = rate_filter / np.sum(rate_filter)
                        binned_spk_rate = signal.filtfilt(rate_filter, 1, rate_per_sp_bin)
                        # instead of plotting a scatter plot of the firing rate at each 
                        # speed bin, plot a log normalised heatmap and overlay results on it
                        
                        spk_binning_edges = np.linspace(np.min(sm_spk_rate), np.max(sm_spk_rate),
                                                                                        len(sp_bin_edges))
                        speed_mesh, spk_mesh = np.meshgrid(sp_bin_edges, spk_binning_edges)
                        binned_rate, _, _ = np.histogram2d(speed_filt, sm_spk_rate, bins=[sp_bin_edges,
                                                                                           spk_binning_edges])
                        #blur the binned rate a bit to make it look nicer
                        sm_binned_rate = blur_image(binned_rate, 5)
                        plt.figure()
                        plt.pcolormesh(speed_mesh, spk_mesh, sm_binned_rate, norm=colors.LogNorm(), alpha=0.5, shading=&#39;flat&#39;, edgecolors=&#39;None&#39;)
                        #overlay the smoothed binned rate against speed
                        plt.hold(True)
                        plt.plot(sp_bin_edges, binned_spk_rate, &#39;r&#39;)
                        #do the linear regression and plot the fit too
                        # TODO: linear regression is broken ie not regressing the correct variables
                        lr = stats.linregress(speed_filt, sm_spk_rate)
                        end_point = lr.intercept + ((sp_bin_edges[-1] - sp_bin_edges[0]) * lr.slope)
                        plt.plot([np.min(sp_bin_edges), np.max(sp_bin_edges)], [lr.intercept, end_point], &#39;r--&#39;)
                        ax = plt.gca()
                        ax.set_xlim(np.min(sp_bin_edges), np.max(sp_bin_edges[-2]))
                        ax.set_ylim(0, np.nanmax(binned_spk_rate) * 1.1)
                        ax.set_ylabel(&#39;Firing rate(Hz)&#39;)
                        ax.set_xlabel(&#39;Running speed(cm/s)&#39;)
                        ax.set_title(&#39;Intercept: {0:.3f}    Slope: {1:.5f}\nPearson: {2:.5f}&#39;.format(lr.intercept, lr.slope, lr.rvalue))
                #do some shuffling of the data to see if the result is signficant            
                if shuffle:                
                        # shift spikes by at least 30 seconds after trial start and
                        # 30 seconds before trial end
                        timeSteps = np.random.randint(30 * posSampRate, nSamples - (30 * posSampRate),
                                                                                                  nShuffles)
                        shuffled_results = []            
                        for t in timeSteps:
                                spk_count = np.roll(spk_hist, t)
                                spk_count_filt = spk_count[~lowSpeedIdx]
                                spk_count_sm = signal.filtfilt(h.ravel(), 1, spk_count_filt)
                                shuffled_results.append(stats.pearsonr(spk_count_sm, speed_filt)[0])
                        if plot:
                                plt.figure()
                                ax = plt.gca()
                                ax.hist(np.abs(shuffled_results), 20)
                                ylims = ax.get_ylim()
                                ax.vlines(res, ylims[0], ylims[1], &#39;r&#39;)
                        
                print(&#34;PPMC: {0}&#34;.format(res[0]))</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.mean_autoCorr"><code class="name flex">
<span>def <span class="ident">mean_autoCorr</span></span>(<span>self, cluster, n=40)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the autocorrelation function mean from 0 to n ms (default=40)
Used to help classify units as principal or interneuron</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean_autoCorr(self, cluster, n=40):
        &#39;&#39;&#39;
        Returns the autocorrelation function mean from 0 to n ms (default=40)
        Used to help classify units as principal or interneuron
        &#39;&#39;&#39;
        if cluster not in self.clusters:
                warnings.warn(&#39;Cluster not available. Try again!&#39;)
        else:
                bins = 201
                Trange = (-500, 500)
                y = self.xcorr(cluster, Trange=Trange)
                counts, bins = np.histogram(y[y != 0], bins=bins, range=Trange)
                mask = np.logical_and(bins&gt;0, bins&lt;n)
                return np.mean(counts[mask])</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.p2t_time"><code class="name flex">
<span>def <span class="ident">p2t_time</span></span>(<span>self, cluster)</span>
</code></dt>
<dd>
<section class="desc"><p>The peak to trough time of a spike in ms</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>cluster</code></strong> :&ensp;<code>int</code></dt>
<dd>the cluster whose waveforms are to be analysed</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>p2t</code></strong> :&ensp;<code>float</code></dt>
<dd>The mean peak-to-trough time for the channel (electrode) that has
the strongest (highest amplitude) signal. Units are ms</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def p2t_time(self, cluster):
        &#34;&#34;&#34;
        The peak to trough time of a spike in ms
        
        Parameters
        ----------
        cluster: int
                the cluster whose waveforms are to be analysed
                
        Returns
        -------
        p2t: float
                The mean peak-to-trough time for the channel (electrode) that has 
                the strongest (highest amplitude) signal. Units are ms
        &#34;&#34;&#34;
        waveforms = self.waveforms[self.cut==cluster, :, :]
        best_chan = np.argmax(np.max(np.mean(waveforms, 0), 1))
        tP = self.getParam(waveforms, param=&#39;tP&#39;)
        tT = self.getParam(waveforms, param=&#39;tT&#39;)
        mn_tP = np.mean(tP, 0)
        mn_tT = np.mean(tT, 0)
        p2t = np.abs(mn_tP[best_chan] - mn_tT[best_chan])
        return p2t * 1000</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.plotClusterSpace"><code class="name flex">
<span>def <span class="ident">plotClusterSpace</span></span>(<span>self, clusters=None, param='Amp', clusts=None, bins=256, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>TODO: aspect of plot boxes in ImageGrid not right as scaled by range of
values now</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plotClusterSpace(self, clusters=None, param=&#39;Amp&#39;, clusts=None, bins=256, **kwargs):
        &#39;&#39;&#39;
        TODO: aspect of plot boxes in ImageGrid not right as scaled by range of
        values now
        &#39;&#39;&#39;
        import tintColours as tcols
        import matplotlib.colors as colors
        from itertools import combinations
        from mpl_toolkits.axes_grid1 import ImageGrid
        
        if isinstance(clusters, int):
                clusters = [clusters]

        amps = self.getParam(param=param)
        bad_electrodes = np.setdiff1d(np.array(range(4)),np.array(np.sum(amps,0).nonzero())[0])
        cmap = np.tile(tcols.colours[0],(bins,1))
        cmap[0] = (1,1,1)
        cmap = colors.ListedColormap(cmap)
        cmap._init()
        alpha_vals = np.ones(cmap.N+3)
        alpha_vals[0] = 0
        cmap._lut[:,-1] = alpha_vals
        cmb = combinations(range(4),2)
        if &#39;fig&#39; in kwargs.keys():
                fig = kwargs[&#39;fig&#39;]
        else:
                fig = plt.figure(figsize=(8,6))
        if &#39;rect&#39; in kwargs.keys():
                rect = kwargs[&#39;rect&#39;]
        else:
                rect = 111
        grid = ImageGrid(fig, rect, nrows_ncols= (2,3), axes_pad=0.1, aspect=False)
        if &#39;Amp&#39; in param:
                myRange = np.vstack((self.scaling*0, self.scaling*2))
        else:
                myRange = None
        clustCMap0 = np.tile(tcols.colours[0],(bins,1))
        clustCMap0[0] = (1,1,1)
        clustCMap0 = colors.ListedColormap(clustCMap0)
        clustCMap0._init()
        clustCMap0._lut[:,-1] = alpha_vals
        for i, c in enumerate(cmb):
                if c not in bad_electrodes:
                        h, ye, xe = np.histogram2d(amps[:,c[0]], amps[:,c[1]], range = myRange[:,c].T, bins=bins)
                        x, y = np.meshgrid(xe[0:-1], ye[0:-1])
                        grid[i].pcolormesh(x, y, h, cmap=clustCMap0, edgecolors=&#39;face&#39;)
                        if clusters is not None:
                                for thisclust in clusters:
                                        clustidx = self.cut == thisclust
                                        h, ye, xe = np.histogram2d(amps[clustidx,c[0]],amps[clustidx,c[1]], range=myRange[:,c].T, bins=bins)
                                        clustCMap = np.tile(tcols.colours[thisclust],(bins,1))
                                        clustCMap[0] = (1,1,1)
                                        clustCMap = colors.ListedColormap(clustCMap)
                                        clustCMap._init()
                                        clustCMap._lut[:,-1] = alpha_vals
                                        grid[i].pcolormesh(x, y, h, cmap=clustCMap, edgecolors=&#39;face&#39;)
                s = str(c[0]+1) + &#39; v &#39; + str(c[1]+1)
                grid[i].text(0.05,0.95, s, va=&#39;top&#39;, ha=&#39;left&#39;, size=&#39;small&#39;, color=&#39;k&#39;, transform=grid[i].transAxes)
                grid[i].set_xlim(xe.min(), xe.max())
                grid[i].set_ylim(ye.min(), ye.max())
        plt.setp([a.get_xticklabels() for a in grid], visible=False)
        plt.setp([a.get_yticklabels() for a in grid], visible=False)
        return fig</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.smoothSpikePosCount"><code class="name flex">
<span>def <span class="ident">smoothSpikePosCount</span></span>(<span>self, x1, npos, sigma=3.0, shuffle=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns a spike train the same length as num pos samples that has been
smoothed in time with a gaussian kernel M in width and standard deviation
equal to sigma</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x1</code></strong> :&ensp;<code>np.array</code></dt>
<dd>The pos indices the spikes occured at</dd>
<dt><strong><code>npos</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of position samples captured</dd>
<dt><strong><code>sigma</code></strong> :&ensp;<code>float</code></dt>
<dd>the standard deviation of the gaussian used to smooth the spike
train</dd>
<dt><strong><code>shuffle</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of seconds to shift the spike train by. Default None</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>smoothed_spikes</code></strong> :&ensp;<code>np.array</code></dt>
<dd>The smoothed spike train</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def smoothSpikePosCount(self, x1, npos, sigma=3.0, shuffle=None):
        &#39;&#39;&#39;
        Returns a spike train the same length as num pos samples that has been
        smoothed in time with a gaussian kernel M in width and standard deviation
        equal to sigma
        
        Parameters
        --------------
        x1 : np.array
                The pos indices the spikes occured at
        npos : int
                The number of position samples captured
        sigma : float
                the standard deviation of the gaussian used to smooth the spike
                train
        shuffle: int
                The number of seconds to shift the spike train by. Default None
        
        Returns
        -----------
        smoothed_spikes : np.array
                The smoothed spike train
        &#39;&#39;&#39;
        spk_hist = np.bincount(x1, minlength=npos)
        if shuffle is not None:
                spk_hist = np.roll(spk_hist, int(shuffle * 50))
        # smooth the spk_hist (which is a temporal histogram) with a 250ms
        # gaussian as with Kropff et al., 2015
        h = signal.gaussian(13, sigma)
        h = h / float(np.sum(h))
        return signal.filtfilt(h.ravel(), 1, spk_hist)</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.thetaBandMaxFreq"><code class="name flex">
<span>def <span class="ident">thetaBandMaxFreq</span></span>(<span>self, x1)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the frequency with the max power in the theta band (6-12Hz)
of a spike trains autocorrelogram. Partly to look for differences
in theta frequency in different running directions a la Blair (Welday paper)</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def thetaBandMaxFreq(self, x1):
        &#39;&#39;&#39;
        Calculates the frequency with the max power in the theta band (6-12Hz)
        of a spike trains autocorrelogram. Partly to look for differences
        in theta frequency in different running directions a la Blair (Welday paper)
        &#39;&#39;&#39;
        y = self.xcorr(x1)
        corr, _ = np.histogram(y[y != 0], bins=201, range=np.array([-500,500]))
        # Take the fft of the spike train autocorr (from -500 to +500ms)
        from scipy.signal import periodogram
        freqs, power = periodogram(corr, fs=200, return_onesided=True)
        power_masked = np.ma.MaskedArray(power,np.logical_or(freqs&lt;6,freqs&gt;12))
        return freqs[np.argmax(power_masked)]</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.thetaModIdx"><code class="name flex">
<span>def <span class="ident">thetaModIdx</span></span>(<span>self, x1)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates a theta modulation index of a spike train based on the cells
autocorrelogram</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x1</code></strong> :&ensp;<code>np.array</code></dt>
<dd>The spike time-series</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>thetaMod</code></strong> :&ensp;<code>float</code></dt>
<dd>The difference of the values at the first peak and trough of the
autocorrelogram</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def thetaModIdx(self, x1):
        &#39;&#39;&#39;
        Calculates a theta modulation index of a spike train based on the cells
        autocorrelogram
        
        Parameters
        ----------
        x1: np.array
                The spike time-series
        Returns
        -------
        thetaMod: float
                The difference of the values at the first peak and trough of the
                autocorrelogram
        &#39;&#39;&#39;
        y = self.xcorr(x1)
        corr, _ = np.histogram(y[y != 0], bins=201, range=np.array([-500,500]))
        # Take the fft of the spike train autocorr (from -500 to +500ms)
        from scipy.signal import periodogram
        freqs, power = periodogram(corr, fs=200, return_onesided=True)
        # Smooth the power over +/- 1Hz
        b = signal.boxcar(3)
        h = signal.filtfilt(b, 3, power)
        
        # Square the amplitude first
        sqd_amp = h ** 2
        # Then find the mean power in the +/-1Hz band either side of that
        theta_band_max_idx = np.nonzero(sqd_amp==np.max(sqd_amp[np.logical_and(freqs&gt;6, freqs&lt;11)]))[0][0]
        mean_theta_band_power = np.mean(sqd_amp[theta_band_max_idx-1:theta_band_max_idx+1])
        # Find the mean amplitude in the 2-50Hz range
        other_band_idx = np.logical_and(freqs&gt;2, freqs&lt;50)
        mean_other_band_power = np.mean(sqd_amp[other_band_idx])
        # Find the ratio of these two - this is the theta modulation index
        return (mean_theta_band_power - mean_other_band_power) / (mean_theta_band_power + mean_other_band_power)</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.thetaModIdxV2"><code class="name flex">
<span>def <span class="ident">thetaModIdxV2</span></span>(<span>self, x1)</span>
</code></dt>
<dd>
<section class="desc"><p>This is a simpler alternative to the thetaModIdx method in that it
calculates the difference between the normalized temporal autocorrelogram
at the trough between 50-70ms and the peak between 100-140ms over
their sum (data is binned into 5ms bins)</p>
<p>Measure used in Cacucci et al., 2004 and Kropff et al 2015</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def thetaModIdxV2(self, x1):
        &#39;&#39;&#39;
        This is a simpler alternative to the thetaModIdx method in that it
        calculates the difference between the normalized temporal autocorrelogram
        at the trough between 50-70ms and the peak between 100-140ms over
        their sum (data is binned into 5ms bins)
        
        Measure used in Cacucci et al., 2004 and Kropff et al 2015
        &#39;&#39;&#39;
        y = self.xcorr(x1)
        corr, bins = np.histogram(y[y != 0], bins=201, range=np.array([-500,500]))
        # &#39;close&#39; the right-hand bin
        bins = bins[0:-1]
        # normalise corr so max is 1.0
        corr = corr/float(np.max(corr))
        thetaAntiPhase = np.min(corr[np.logical_and(bins&gt;50,bins&lt;70)])
        thetaPhase = np.max(corr[np.logical_and(bins&gt;100, bins&lt;140)])
        return (thetaPhase-thetaAntiPhase) / (thetaPhase+thetaAntiPhase)</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.trial_av_firing_rate"><code class="name flex">
<span>def <span class="ident">trial_av_firing_rate</span></span>(<span>self, cluster)</span>
</code></dt>
<dd>
<section class="desc"><p>returns the trial average firing rate of a cluster in Hz</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def trial_av_firing_rate(self, cluster):
        &#39;&#39;&#39;
        returns the trial average firing rate of a cluster in Hz
        &#39;&#39;&#39;
        return self.getNSpikes(cluster) / float(self.header[&#39;duration&#39;])</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.xcorr"><code class="name flex">
<span>def <span class="ident">xcorr</span></span>(<span>self, x1, x2=None, Trange=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the histogram of the ISIs</p>
<h2 id="parameters">Parameters</h2>
<p>x1 - 1d np.array list of spike times
x2 - (optional) 1d np.array of spike times
Trange - 1x2 np.array for range of times to bin up. Defaults
to [-500, +500]</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def xcorr(self, x1, x2=None, Trange=None):
        &#39;&#39;&#39;
        Returns the histogram of the ISIs

        Parameters
        ---------------
        x1 - 1d np.array list of spike times
        x2 - (optional) 1d np.array of spike times
        Trange - 1x2 np.array for range of times to bin up. Defaults
                                to [-500, +500]
        &#39;&#39;&#39;
        if x2 is None:
                x2 = x1.copy()
        if Trange is None:
                Trange = np.array([-500, 500])
        y = []
        irange = x1[:, np.newaxis] + Trange[np.newaxis, :]
        dts = np.searchsorted(x2, irange)
        for i, t in enumerate(dts):
                y.extend(x2[t[0]:t[1]] - x1[i])
        y = np.array(y, dtype=float)
        return y</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ephysiopy.dacq2py" href="index.html">ephysiopy.dacq2py</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ephysiopy.dacq2py.spikecalcs.SpikeCalcs" href="#ephysiopy.dacq2py.spikecalcs.SpikeCalcs">SpikeCalcs</a></code></h4>
<ul class="">
<li><code><a title="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.clusterQuality" href="#ephysiopy.dacq2py.spikecalcs.SpikeCalcs.clusterQuality">clusterQuality</a></code></li>
<li><code><a title="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.getMeanWaveform" href="#ephysiopy.dacq2py.spikecalcs.SpikeCalcs.getMeanWaveform">getMeanWaveform</a></code></li>
<li><code><a title="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.getNSpikes" href="#ephysiopy.dacq2py.spikecalcs.SpikeCalcs.getNSpikes">getNSpikes</a></code></li>
<li><code><a title="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.getParam" href="#ephysiopy.dacq2py.spikecalcs.SpikeCalcs.getParam">getParam</a></code></li>
<li><code><a title="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.half_amp_dur" href="#ephysiopy.dacq2py.spikecalcs.SpikeCalcs.half_amp_dur">half_amp_dur</a></code></li>
<li><code><a title="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.ifr_sp_corr" href="#ephysiopy.dacq2py.spikecalcs.SpikeCalcs.ifr_sp_corr">ifr_sp_corr</a></code></li>
<li><code><a title="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.mean_autoCorr" href="#ephysiopy.dacq2py.spikecalcs.SpikeCalcs.mean_autoCorr">mean_autoCorr</a></code></li>
<li><code><a title="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.p2t_time" href="#ephysiopy.dacq2py.spikecalcs.SpikeCalcs.p2t_time">p2t_time</a></code></li>
<li><code><a title="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.plotClusterSpace" href="#ephysiopy.dacq2py.spikecalcs.SpikeCalcs.plotClusterSpace">plotClusterSpace</a></code></li>
<li><code><a title="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.smoothSpikePosCount" href="#ephysiopy.dacq2py.spikecalcs.SpikeCalcs.smoothSpikePosCount">smoothSpikePosCount</a></code></li>
<li><code><a title="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.thetaBandMaxFreq" href="#ephysiopy.dacq2py.spikecalcs.SpikeCalcs.thetaBandMaxFreq">thetaBandMaxFreq</a></code></li>
<li><code><a title="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.thetaModIdx" href="#ephysiopy.dacq2py.spikecalcs.SpikeCalcs.thetaModIdx">thetaModIdx</a></code></li>
<li><code><a title="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.thetaModIdxV2" href="#ephysiopy.dacq2py.spikecalcs.SpikeCalcs.thetaModIdxV2">thetaModIdxV2</a></code></li>
<li><code><a title="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.trial_av_firing_rate" href="#ephysiopy.dacq2py.spikecalcs.SpikeCalcs.trial_av_firing_rate">trial_av_firing_rate</a></code></li>
<li><code><a title="ephysiopy.dacq2py.spikecalcs.SpikeCalcs.xcorr" href="#ephysiopy.dacq2py.spikecalcs.SpikeCalcs.xcorr">xcorr</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.5</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>