<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.5" />
<title>ephysiopy.dacq2py.fieldcalcs API documentation</title>
<meta name="description" content="Created on Mon Jun 18 18:31:31 2012 â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ephysiopy.dacq2py.fieldcalcs</code></h1>
</header>
<section id="section-intro">
<p>Created on Mon Jun 18 18:31:31 2012</p>
<p>@author: robin</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;
Created on Mon Jun 18 18:31:31 2012

@author: robin
&#34;&#34;&#34;
import numpy as np
from scipy import signal, spatial, misc, ndimage
import skimage as skimage
import matplotlib.pyplot as plt
import warnings

class FieldCalcs:
        &#39;&#39;&#39;
        A series of methods are defined here that allow quantification of ratemaps
        Methods:
                - _blur_image(im, n, ny=None, ftype=&#39;boxcar&#39;)
                - limit_to_one(rmap, prc=50, min_dist=5)
                - global_threshold
                - local_threshold
                - get_field_props
                - coherence(smoothed_rate, unsmoothed rate)
                - kldiv_dir(polarplot)
                - kldiv(X, pvect1,pvect2,variant=None)
                - skaggsInfo(ratemap, dwelltimes)
                - xPearson(ratemap1,ratemap2=None,mode=&#39;full&#39;)
                - linearStackAverage(ratemap)
        &#39;&#39;&#39;
        def _blur_image(self, im, n, ny=None, ftype=&#39;boxcar&#39;):
                &#34;&#34;&#34; blurs the image by convolving with a filter (&#39;gaussian&#39; or
                        &#39;boxcar&#39;) of
                        size n. The optional keyword argument ny allows for a different
                        size in the y direction.
                &#34;&#34;&#34;
                n = int(n)
                if not ny:
                        ny = n
                else:
                        ny = int(ny)
                #  keep track of nans
                nan_idx = np.isnan(im)
                im[nan_idx] = 0
                if ftype == &#39;boxcar&#39;:
                        if np.ndim(im) == 1:
                                g = signal.boxcar(n) / float(n)
                        elif np.ndim(im) == 2:
                                g = signal.boxcar([n, ny]) / float(n)
                elif ftype == &#39;gaussian&#39;:
                        x, y = np.mgrid[-n:n+1, -ny:ny+1]
                        g = np.exp(-(x**2/float(n) + y**2/float(ny)))
                        g = g / g.sum()
                        if np.ndim(im) == 1:
                                g = g[n, :]
                improc = signal.convolve(im, g, mode=&#39;same&#39;)
                improc[nan_idx] = np.nan
                return improc    
        
        def limit_to_one(self, A, prc=50, min_dist=5):
                &#34;&#34;&#34;
                Processes a multi-peaked ratemap (ie grid cell) and returns a matrix
                where the multi-peaked ratemap consist of a single peaked field that is
                a) not connected to the border and b) close to the middle of the ratemap
                &#34;&#34;&#34;
                Ac = A.copy()
                Ac[np.isnan(A)] = 0
                # smooth Ac more to remove local irregularities
                n = ny = 5
                x, y = np.mgrid[-n:n+1, -ny:ny+1]
                g = np.exp(-(x**2/float(n) + y**2/float(ny)))
                g = g / g.sum()
                Ac = signal.convolve(Ac, g, mode=&#39;same&#39;)
                peak_mask = skimage.feature.peak_local_max(Ac, min_distance=min_dist,
                                                                                                   exclude_border=False,
                                                                                                   indices=False)
                peak_labels = skimage.measure.label(peak_mask, 8)
                field_labels = skimage.morphology.watershed(image=-Ac,
                                                                                                        markers=peak_labels)
                nFields = np.max(field_labels)
                sub_field_mask = np.zeros((nFields, Ac.shape[0], Ac.shape[1]))
                labelled_sub_field_mask = np.zeros_like(sub_field_mask)
                sub_field_props = skimage.measure.regionprops(field_labels,
                                                                                                          intensity_image=Ac)
                sub_field_centroids = []
                sub_field_size = []

                for sub_field in sub_field_props:
                        tmp = np.zeros(Ac.shape).astype(bool)
                        tmp[sub_field.coords[:, 0], sub_field.coords[:, 1]] = True
                        tmp2 = Ac &gt; sub_field.max_intensity * (prc/float(100))
                        sub_field_mask[sub_field.label - 1, :, :] = np.logical_and(tmp2, tmp)
                        labelled_sub_field_mask[sub_field.label-1, np.logical_and(tmp2, tmp)] = sub_field.label
                        sub_field_centroids.append(sub_field.centroid)
                        sub_field_size.append(sub_field.area)  # in bins
                sub_field_mask = np.sum(sub_field_mask, 0)
                middle = np.round(np.array(A.shape) / 2)
                normd_dists = sub_field_centroids - middle
                field_dists_from_middle = np.hypot(normd_dists[:, 0], normd_dists[:, 1])
                central_field_idx = np.argmin(field_dists_from_middle)
                central_field = np.squeeze(labelled_sub_field_mask[central_field_idx, :, :])
                # collapse the labelled mask down to an 2d array
                labelled_sub_field_mask = np.sum(labelled_sub_field_mask, 0)
                # clear the border
                cleared_mask = skimage.segmentation.clear_border(central_field)
                # check we&#39;ve still got stuff in the matrix or fail
                if ~np.any(cleared_mask):
                        print(&#39;No fields were detected away from edges so nothing returned&#39;)
                        return None, None, None
                else:
                        central_field_props = sub_field_props[central_field_idx]
                        return central_field_props, central_field, central_field_idx

        def global_threshold(self, A, prc=50, min_dist=5):
                &#39;&#39;&#39;
                Globally thresholds a ratemap and counts number of fields found
                &#39;&#39;&#39;
                Ac = A.copy()
                Ac[np.isnan(A)] = 0
                n = ny = 5
                x, y = np.mgrid[-n:n+1, -ny:ny+1]
                g = np.exp(-(x**2/float(n) + y**2/float(ny)))
                g = g / g.sum()
                Ac = signal.convolve(Ac, g, mode=&#39;same&#39;)
                maxRate = np.nanmax(np.ravel(Ac))
                Ac[Ac &lt; maxRate*(prc/float(100))] = 0
                peak_mask = skimage.feature.peak_local_max(Ac, min_distance=min_dist,
                                                                                                   exclude_border=False,
                                                                                                   indices=False)
                peak_labels = skimage.measure.label(peak_mask, 8)
                field_labels = skimage.morphology.watershed(image=-Ac,
                                                                                                        markers=peak_labels)
                nFields = np.max(field_labels)
                return nFields

        def local_threshold(self, A, prc=50, min_dist=5):
                &#34;&#34;&#34;
                Locally thresholds a ratemap to take only the surrounding prc amount
                around any local peak
                &#34;&#34;&#34;
                Ac = A.copy()
                nanidx = np.isnan(Ac)
                Ac[nanidx] = 0
                # smooth Ac more to remove local irregularities
                n = ny = 5
                x, y = np.mgrid[-n:n+1, -ny:ny+1]
                g = np.exp(-(x**2/float(n) + y**2/float(ny)))
                g = g / g.sum()
                Ac = signal.convolve(Ac, g, mode=&#39;same&#39;)
                peak_mask = skimage.feature.peak_local_max(Ac, min_distance=min_dist,
                                                                                                   exclude_border=False,
                                                                                                   indices=False)
                peak_labels = skimage.measure.label(peak_mask, 8)
                field_labels = skimage.morphology.watershed(image=-Ac,
                                                                                                        markers=peak_labels)
                nFields = np.max(field_labels)
                sub_field_mask = np.zeros((nFields, Ac.shape[0], Ac.shape[1]))
                sub_field_props = skimage.measure.regionprops(field_labels,
                                                                                                          intensity_image=Ac)
                sub_field_centroids = []
                sub_field_size = []

                for sub_field in sub_field_props:
                        tmp = np.zeros(Ac.shape).astype(bool)
                        tmp[sub_field.coords[:, 0], sub_field.coords[:, 1]] = True
                        tmp2 = Ac &gt; sub_field.max_intensity * (prc/float(100))
                        sub_field_mask[sub_field.label - 1, :, :] = np.logical_and(tmp2, tmp)
                        sub_field_centroids.append(sub_field.centroid)
                        sub_field_size.append(sub_field.area)  # in bins
                sub_field_mask = np.sum(sub_field_mask, 0)
                A_out = np.zeros_like(A)
                A_out[sub_field_mask.astype(bool)] = A[sub_field_mask.astype(bool)]
                A_out[nanidx] = np.nan
                return A_out

        def getBorderScore(self, A, B=None, shape=&#39;square&#39;, fieldThresh=0.3, smthKernSig=3,
                                        circumPrc=0.2, binSize=3.0, minArea=200, debug=False):
                &#39;&#39;&#39;
                Calculates a border score totally dis-similar to that calculated in Solstad et al
                (2008)

                Parameters
                ----------
                A : np.array
                        Should be the ratemap
                B : np.array
                        This should be a boolean mask where True (1)
                        is equivalent to the presence of a border and False (0)
                        is equivalent to &#39;open space&#39;. Naievely this will be the 
                        edges of the ratemap but could be used to take account of 
                        boundary insertions/ creations to check tuning to multiple
                        environmental boundaries. Default None: when the mask is 
                        None then a mask is created that has 1&#39;s at the edges of the
                        ratemap i.e. it is assumed that occupancy = environmental 
                        shape
                shape : str
                        description of environment shape. Currently
                        only &#39;square&#39; or &#39;circle&#39; accepted. Used to calculate the
                        proportion of the environmental boundaries to examine for
                        firing
                fieldThresh : float
                        Between 0 and 1 this is the percentage 
                        amount of the maximum firing rate
                        to remove from the ratemap (i.e. to remove noise)
                smthKernSig : float
                        the sigma value used in smoothing the ratemap
                        (again!) with a gaussian kernel
                circumPrc : float
                        The percentage amount of the circumference
                        of the environment that the field needs to be to count
                        as long enough to make it through
                binSize : float
                        bin size in cm
                minArea : float
                        min area for a field to be considered
                debug : bool
                        If True then some plots and text will be output

                Returns
                -------
                        float : the border score

                Notes
                -----
                If the cell is a border cell (BVC) then we know that it should
                fire at a fixed distance from a given boundary (possibly more
                than one). In essence this algorithm estimates the amount of 
                variance in this distance i.e. if the cell is a border cell this
                number should be small. This is achieved by first doing a bunch of 
                morphological operations to isolate individual fields in the
                ratemap (similar to the code used in phasePrecession.py - see 
                the partitionFields method therein). These partitioned fields are then
                thinned out (using skimage&#39;s skeletonize) to a single pixel
                wide field which will lie more or less in the middle of the
                (highly smoothed) sub-field. It is the variance in distance from the
                nearest boundary along this pseudo-iso-line that is the boundary measure

                Other things to note are that the pixel-wide field has to have some minimum
                length. In the case of a circular environment this is set to 
                20% of the circumference; in the case of a square environment markers
                this is at least half the length of the longest side

                &#39;&#39;&#39;

                dwell = np.isfinite(A)
                # need to know borders of the environment so we can see if a field
                # touches the edges, and the perimeter length of the environment
                # deal with square or circles differently
                borderMask = np.zeros_like(A)
                A_rows, A_cols = np.shape(A)
                if &#39;circle&#39; in shape:
                        radius = np.max(np.array(np.shape(A))) / 2.0
                        dist_mask = skimage.morphology.disk(radius)
                        if np.shape(dist_mask) &gt; np.shape(A):
                                dist_mask = dist_mask[1:A_rows+1, 1:A_cols+1]
                        tmp = np.zeros([A_rows + 2, A_cols + 2])
                        tmp[1:-1, 1:-1] = dist_mask
                        dists = ndimage.morphology.distance_transform_bf(tmp)
                        dists = dists[1:-1, 1:-1]
                        perimeter =  2.0 * radius * np.pi
                        borderMask = np.logical_xor(dists &lt;= 0, dists &lt; 2)
                        # open up the border mask a little
                        borderMask = skimage.morphology.binary_dilation(borderMask, skimage.morphology.disk(1))
                elif &#39;square&#39; in shape:
                        perimeter = np.sum(np.array(np.shape(A)*2))
                        borderMask[0:3, :] = 1
                        borderMask[-3:, :] = 1
                        borderMask[:, 0:3] = 1
                        borderMask[:, -3:] = 1
                        tmp = np.zeros([A_rows + 2, A_cols + 2])
                        dist_mask = np.ones_like(A)
                        tmp[1:-1, 1:-1] = dist_mask
                        dists = ndimage.morphology.distance_transform_bf(tmp)
                        # remove edges to make same shape as input ratemap
                        dists = dists[1:-1, 1:-1]
                A[np.isnan(A)] = 0
                # get some morphological info about the fields in the ratemap
                # start image processing:
                # get some markers
                # NB I&#39;ve tried a variety of techniques to optimise this part and the
                # best seems to be the local adaptive thresholding technique which)
                # smooths locally with a gaussian - see the skimage docs for more
                idx = A &gt;= np.nanmax(np.ravel(A)) * fieldThresh         
                A_thresh = np.zeros_like(A)
                A_thresh[idx] = A[idx]

                # label these markers so each blob has a unique id
                labels, nFields = ndimage.label(A_thresh)
                # remove small objects
                min_size = int(minArea / binSize) - 1
                if debug:
                        plt.figure()
                        plt.imshow(A_thresh)
                        ax = plt.gca()
                        ax.set_title(&#39;Before removing small objects&#39;)
                skimage.morphology.remove_small_objects(labels, min_size=min_size, connectivity=2, in_place=True)
                labels = skimage.segmentation.relabel_sequential(labels)[0]
                nFields = np.max(labels)
                if nFields == 0:
                        return np.nan
                # Iterate over the labelled parts of the array labels calculating
                # how much of the total circumference of the environment edge it
                # covers

                fieldAngularCoverage = np.zeros([1, nFields]) * np.nan
                fractionOfPixelsOnBorder = np.zeros([1, nFields]) * np.nan
                fieldsToKeep = np.zeros_like(A)
                for i in range(1, nFields+1):
                        fieldMask = np.logical_and(labels==i, borderMask)

                        # check the angle subtended by the fieldMask
                        if np.sum(fieldMask.astype(int)) &gt; 0:
                                s = skimage.measure.regionprops(fieldMask.astype(int), intensity_image=A_thresh)[0]
                                x = s.coords[:,0] - (A_cols / 2.0)
                                y = s.coords[:,1] - (A_rows / 2.0)
                                subtended_angle = np.rad2deg(np.ptp(np.arctan2(x,y)))
                                if subtended_angle &gt; (360 * circumPrc):
                                        pixelsOnBorder = np.count_nonzero(fieldMask) / float(np.count_nonzero(labels==i))
                                        fractionOfPixelsOnBorder[:,i-1] = pixelsOnBorder
                                        if pixelsOnBorder &gt; 0.5:
                                                fieldAngularCoverage[0, i-1] = subtended_angle

                                fieldsToKeep = np.logical_or(fieldsToKeep, labels==i)
                if debug:
                        fig, ax = plt.subplots(4,1,figsize=(3,9))
                        ax1 = ax[0]
                        ax2 = ax[1]
                        ax3 = ax[2]
                        ax4 = ax[3]
                        ax1.imshow(A)
                        ax2.imshow(labels)
                        ax3.imshow(A_thresh)
                        ax4.imshow(fieldsToKeep)
                        plt.show()
                        for i,f in enumerate(fieldAngularCoverage.ravel()):
                                print(&#34;angle subtended by field {0} = {1:.2f}&#34;.format(i+1, f))
                        for i,f in enumerate(fractionOfPixelsOnBorder.ravel()):
                                print(&#34;% pixels on border for field {0} = {1:.2f}&#34;.format(i+1, f))
                fieldAngularCoverage = (fieldAngularCoverage / 360.)
                if np.sum(fieldsToKeep) == 0:
                        return np.nan
                rateInField = A[fieldsToKeep]
                # normalize firing rate in the field to sum to 1
                rateInField = rateInField / np.nansum(rateInField)
                dist2WallInField = dists[fieldsToKeep]
                Dm = np.dot(dist2WallInField, rateInField)
                if &#39;circle&#39; in shape:
                        Dm = Dm / radius
                elif &#39;square&#39; in shape:
                        Dm = Dm / (np.max(np.shape(A)) / 2.0)
                borderScore = (fractionOfPixelsOnBorder-Dm) / (fractionOfPixelsOnBorder+Dm)
                return np.max(borderScore)

        def get_field_props(self, A, min_dist=5, neighbours=2, prc=50,
                                                plot=False, ax=None, tri=False, verbose=True, **kwargs):
                &#34;&#34;&#34;
                Returns a dictionary of properties of the field(s) in a ratemap A
                Parameters
                ----------
                A : numpy.array
                        a ratemap (but could be any image)
                min_dist : float
                        the separation (in bins) between fields for measures
                        such as field distance to make sense. Used to
                        partition the image into separate fields in the call to
                        skimage.feature.peak_local_max
                neighbours : int
                        the number of fields to consider as neighbours to
                        any given field. Defaults to 2
                prc : float
                        percent of fields to consider
                ax : matplotlib.Axes
                        user supplied axis. If None a new figure window is created
                tri : bool
                        whether to do Delaunay triangulation between fields
                        and add to plot
                verbose : bool
                        dumps the properties to the console
                plot : bool
                        whether to plot some output - currently consists of the
                        ratemap A, the fields of which are outline in a black
                        contour. Default False

                Returns
                -------
                result : dict
                        The properties of the field(s) in the input ratemap A
                &#34;&#34;&#34;

                from scipy.spatial import Delaunay
                from skimage.measure import find_contours
                from sklearn.neighbors import NearestNeighbors
                import gridcell
                import matplotlib.cm as cm
                nan_idx = np.isnan(A)
                Ac = A.copy()
                Ac[np.isnan(A)] = 0
                # smooth Ac more to remove local irregularities
                n = ny = 5
                x, y = np.mgrid[-n:n+1, -ny:ny+1]
                g = np.exp(-(x**2/float(n) + y**2/float(ny)))
                g = g / g.sum()
                Ac = signal.convolve(Ac, g, mode=&#39;same&#39;)
                if &#39;clear_border&#39; in kwargs.keys():
                        clear_border = True
                else:
                        clear_border = False
                peak_idx = skimage.feature.peak_local_max(Ac, min_distance=min_dist,
                                                                                                  exclude_border=clear_border,
                                                                                                  indices=True)
                if neighbours &gt; len(peak_idx):
                        print(&#39;neighbours value of {0} &gt; the {1} peaks found&#39;.format(neighbours, len(peak_idx)))
                        print(&#39;Reducing neighbours to number of peaks found&#39;)
                        neighbours = len(peak_idx)
                peak_mask = skimage.feature.peak_local_max(Ac, min_distance=min_dist, exclude_border=clear_border,
                                                                                                   indices=False)
                peak_labels = skimage.measure.label(peak_mask, 8)
                field_labels = skimage.morphology.watershed(image=-Ac,
                                                                                                        markers=peak_labels)
                nFields = np.max(field_labels)
                sub_field_mask = np.zeros((nFields, Ac.shape[0], Ac.shape[1]))
                sub_field_props = skimage.measure.regionprops(field_labels,
                                                                                                          intensity_image=Ac)
                sub_field_centroids = []
                sub_field_size = []

                for sub_field in sub_field_props:
                        tmp = np.zeros(Ac.shape).astype(bool)
                        tmp[sub_field.coords[:, 0], sub_field.coords[:, 1]] = True
                        tmp2 = Ac &gt; sub_field.max_intensity * (prc/float(100))
                        sub_field_mask[sub_field.label - 1, :, :] = np.logical_and(tmp2, tmp)
                        sub_field_centroids.append(sub_field.centroid)
                        sub_field_size.append(sub_field.area)  # in bins
                sub_field_mask = np.sum(sub_field_mask, 0)
                contours = skimage.measure.find_contours(sub_field_mask, 0.5)
                # find the nearest neighbors to the peaks of each sub-field


                nbrs = NearestNeighbors(n_neighbors=neighbours, algorithm=&#39;ball_tree&#39;).fit(peak_idx)
                distances, indices = nbrs.kneighbors(peak_idx)
                mean_field_distance = np.mean(distances[:, 1:neighbours])


                nValid_bins = np.sum(~nan_idx)
                # calculate the amount of out of field firing
                A_non_field = np.zeros_like(A) * np.nan
                A_non_field[~sub_field_mask.astype(bool)] = A[~sub_field_mask.astype(bool)]
                A_non_field[nan_idx] = np.nan
                out_of_field_firing_prc = (np.count_nonzero(A_non_field &gt; 0) / float(nValid_bins)) * 100
                Ac[np.isnan(A)] = np.nan
                &#34;&#34;&#34;
                get some stats about the field ellipticity
                &#34;&#34;&#34;
                central_field_props, central_field, central_field_idx = self.limit_to_one(A, prc=50)
                if central_field is None:
                        ellipse_ratio = np.nan
                else:
                        contour_coords = find_contours(central_field, 0.5)
                        G = gridcell.SAC()
                        a = G.__fit_ellipse__(contour_coords[0][:,0], contour_coords[0][:,1])
                        ellipse_axes = G.__ellipse_axis_length__(a)
                        ellipse_ratio = np.min(ellipse_axes) / np.max(ellipse_axes)
                &#39;&#39;&#39; using the peak_idx values calculate the angles of the triangles that
                make up a delaunay tesselation of the space if the calc_angles arg is
                in kwargs
                &#39;&#39;&#39;
                if &#39;calc_angs&#39; in kwargs.keys():
                        try:
                                angs = self.calc_angs(peak_idx)
                        except:
                                angs = np.nan
                else:
                        angs = None

                if plot:
                        if ax is None:
                                fig = plt.figure()
                                ax = fig.add_subplot(111)
                        else:
                                ax = ax
                        Am = np.ma.MaskedArray(Ac, mask=nan_idx, copy=True)
                        ax.pcolormesh(Am, cmap=cm.jet, edgecolors=&#39;face&#39;)
                        for c in contours:
                                ax.plot(c[:, 1], c[:, 0], &#39;k&#39;)
                        # do the delaunay magic
                        if tri:
                                tri = Delaunay(peak_idx)
                                ax.triplot(peak_idx[:,1], peak_idx[:,0], tri.simplices.copy(), color=&#39;w&#39;, marker=&#39;o&#39;)
                        ax.set_xlim(0, Ac.shape[1] - 0.5)
                        ax.set_ylim(0, Ac.shape[0] - 0.5)
                        ax.set_xticklabels(&#39;&#39;)
                        ax.set_yticklabels(&#39;&#39;)
                        ax.invert_yaxis()
                props = {&#39;Ac&#39; : Ac,
                                 &#39;Peak_rate&#39;: np.nanmax(A),
                                 &#39;Mean_rate&#39;: np.nanmean(A),
                                 &#39;Field_size&#39;: np.mean(sub_field_size),
                                 &#39;Pct_bins_with_firing&#39;: (np.sum(sub_field_mask) / nValid_bins) * 100,
                                 &#39;Out_of_field_firing_prc&#39;: out_of_field_firing_prc,
                                 &#39;Dist_between_fields&#39;: mean_field_distance,
                                 &#39;Num_fields&#39;: float(nFields),
                                 &#39;Sub_field_mask&#39;: sub_field_mask,
                                 &#39;Smoothed_map&#39;: Ac,
                                 &#39;field_labels&#39;: field_labels,
                                 &#39;Peak_idx&#39;: peak_idx,
                                 &#39;angles&#39;: angs,
                                 &#39;contours&#39;: contours,
                                 &#39;ellipse_ratio&#39;: ellipse_ratio}

                if verbose:
                        print(&#39;\nPercentage of bins with firing: {:.2%}&#39;.format(np.sum(sub_field_mask) / nValid_bins))
                        print(&#39;Percentage out of field firing: {:.2%}&#39;.format(np.count_nonzero(A_non_field &gt; 0) / float(nValid_bins)))
                        print(&#39;Peak firing rate: {:.3} Hz&#39;.format(np.nanmax(A)))
                        print(&#39;Mean firing rate: {:.3} Hz&#39;.format(np.nanmean(A)))
                        print(&#39;Number of fields: {0}&#39;.format(nFields))
                        print(&#39;Mean field size: {:.5} cm&#39;.format(np.mean(sub_field_size)))  # 3 is binsize)
                        print(&#39;Mean inter-peak distance between fields: {:.4} cm&#39;.format(mean_field_distance))
                return props

        def calc_angs(self, points):
                &#34;&#34;&#34;
                Calculates the angles for all triangles in a delaunay tesselation of
                the peak points in the ratemap
                &#34;&#34;&#34;

                # calculate the lengths of the sides of the triangles
                sideLen = np.hypot(points[:, 0], points[:, 1])
                tri = spatial.Delaunay(points)
                indices, indptr = tri.vertex_neighbor_vertices
                nTris = tri.nsimplex
                outAngs = []
                for k in range(nTris):
                        idx = indptr[indices[k]:indices[k+1]]
                        a = sideLen[k]
                        b = sideLen[idx[0]]
                        c = sideLen[idx[1]]
                        angA = self._getAng(a, b, c)
                        angB = self._getAng(b, c, a)
                        angC = self._getAng(c, a, b)
                        outAngs.append((angA, angB, angC))
                return np.array(outAngs).T

        def _getAng(self, a, b, c):
                &#39;&#39;&#39;
                Given lengths a,b,c of the sides of a triangle this returns the angles
                in degress of all 3 angles
                &#39;&#39;&#39;
                return np.degrees(np.arccos((c**2 - b**2 - a**2)/(-2.0 * a * b)))

        def corr_maps(self, map1, map2, maptype=&#39;normal&#39;):
                &#39;&#39;&#39;
                correlates two ratemaps together ignoring areas that have zero sampling
                &#39;&#39;&#39;
                if map1.shape &gt; map2.shape:
                        map2 = misc.imresize(map2, map1.shape, interp=&#39;nearest&#39;, mode=&#39;F&#39;)
                elif map1.shape &lt; map2.shape:
                        map1 = misc.imresize(map1, map2.shape, interp=&#39;nearest&#39;, mode=&#39;F&#39;)
                map1 = map1.flatten()
                map2 = map2.flatten()
                if maptype is &#39;normal&#39;:
                        valid_map1 = np.logical_or((map1 &gt; 0), ~np.isnan(map1))
                        valid_map2 = np.logical_or((map2 &gt; 0), ~np.isnan(map2))
                elif maptype is &#39;grid&#39;:
                        valid_map1 = ~np.isnan(map1)
                        valid_map2 = ~np.isnan(map2)
                valid = np.logical_and(valid_map1, valid_map2)
                r = np.corrcoef(map1[valid], map2[valid])
                if r.any():
                        return r[1][0]
                else:
                        return np.nan

        def coherence(self, smthd_rate, unsmthd_rate):
                &#39;&#39;&#39;calculates coherence of receptive field via correlation of smoothed
                and unsmoothed ratemaps
                &#39;&#39;&#39;
                smthd = smthd_rate.ravel()
                unsmthd = unsmthd_rate.ravel()
                si = ~np.isnan(smthd)
                ui = ~np.isnan(unsmthd)
                idx = ~(~si | ~ui)
                coherence = np.corrcoef(unsmthd[idx], smthd[idx])
                return coherence[1,0]

        def kldiv_dir(self, polarPlot):
                &#34;&#34;&#34;
                Returns a kl divergence for directional firing: measure of directionality.
                Calculates kl diveregence between a smoothed ratemap (probably should be smoothed
                otherwise information theoretic measures don&#39;t &#39;care&#39; about position of bins relative to
                one another) and a pure circular distribution. The larger the divergence the more
                tendancy the cell has to fire when the animal faces a specific direction.

                Parameters
                ----------
                polarPlot: 1D-array
                        The binned and smoothed directional ratemap

                Returns
                -------
                klDivergence: float
                        The divergence from circular of the 1D-array from a uniform circular
                        distribution
                &#34;&#34;&#34;

                __inc = 0.00001
                polarPlot = np.atleast_2d(polarPlot)
                polarPlot[np.isnan(polarPlot)] = __inc
                polarPlot[polarPlot == 0] = __inc
                normdPolar = polarPlot / float(np.nansum(polarPlot))
                nDirBins = polarPlot.shape[1]
                compCirc = np.ones_like(polarPlot) / float(nDirBins)
                kldivergence = self.kldiv(np.arange(0,nDirBins), normdPolar, compCirc)
                return kldivergence

        def kldiv(self, X, pvect1, pvect2, variant=None):
                &#39;&#39;&#39;
                Calculates the Kullback-Leibler or Jensen-Shannon divergence between two distributions.

                kldiv(X,P1,P2) returns the Kullback-Leibler divergence between two
                distributions specified over the M variable values in vector X.  P1 is a
                length-M vector of probabilities representing distribution 1, and P2 is a
                length-M vector of probabilities representing distribution 2.  Thus, the
                probability of value X(i) is P1(i) for distribution 1 and P2(i) for
                distribution 2.  The Kullback-Leibler divergence is given by:

                .. math:: KL(P1(x),P2(x)) = sum_[P1(x).log(P1(x)/P2(x))]

                If X contains duplicate values, there will be an warning message, and these
                values will be treated as distinct values.  (I.e., the actual values do
                not enter into the computation, but the probabilities for the two
                duplicate values will be considered as probabilities corresponding to
                two unique values.)  The elements of probability vectors P1 and P2 must
                each sum to 1 +/- .00001.

                kldiv(X,P1,P2,&#39;sym&#39;) returns a symmetric variant of the Kullback-Leibler
                divergence, given by [KL(P1,P2)+KL(P2,P1)]/2 [1]_

                kldiv(X,P1,P2,&#39;js&#39;) returns the Jensen-Shannon divergence, given by
                [KL(P1,Q)+KL(P2,Q)]/2, where Q = (P1+P2)/2.  See the Wikipedia article
                for &#34;Kullbackâ€“Leibler divergence&#34;.  This is equal to 1/2 the so-called
                &#34;Jeffrey divergence.&#34; [2]_

                References
                ----------
                .. [1] Johnson, D.H. and S. Sinanovic. &#34;Symmetrizing the Kullback-Leibler
                distance.&#34; IEEE Transactions on Information Theory (Submitted).
                .. [2] Rubner, Y., Tomasi, C., and Guibas, L. J., 2000. &#34;The Earth Mover&#39;s
                distance as a metric for image retrieval.&#34; International Journal of
                Computer Vision, 40(2): 99-121.

                See Also
                --------
                Cover, T.M. and J.A. Thomas. &#34;Elements of Information Theory,&#34; Wiley, 1991.

                https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence

                Notes
                -----
                This function is taken from one on the Mathworks file exchange
                &#39;&#39;&#39;

                if not np.equal(np.unique(X), np.sort(X)).all():
                        warnings.warn(&#39;X contains duplicate values. Treated as distinct values.&#39;,
                                                  UserWarning)
                if not np.equal(np.shape(X), np.shape(pvect1)).all() or not np.equal(np.shape(X), np.shape(pvect2)).all():
                        warnings.warn(&#39;All inputs must have the same dimension.&#39;, UserWarning)
                if (np.abs(np.sum(pvect1) - 1) &gt; 0.00001) or (np.abs(np.sum(pvect2) - 1) &gt; 0.00001):
                        warnings.warn(&#39;Probabilities don&#39;&#39;t sum to 1.&#39;, UserWarning)
                if variant:
                        if variant == &#39;js&#39;:
                                logqvect = np.log2((pvect2 + pvect1) / 2)
                                KL = 0.5 * (np.nansum(pvect1 * (np.log2(pvect1) - logqvect)) + np.sum(pvect2 * (np.log2(pvect2) - logqvect)))
                                return KL
                        elif variant == &#39;sym&#39;:
                                KL1 = np.nansum(pvect1 * (np.log2(pvect1) - np.log2(pvect2)))
                                KL2 = np.nansum(pvect2 * (np.log2(pvect2) - np.log2(pvect1)))
                                KL = (KL1 + KL2) / 2
                                return KL
                        else:
                                warnings.warn(&#39;Last argument not recognised&#39;, UserWarning)
                KL = np.nansum(pvect1 * (np.log2(pvect1) - np.log2(pvect2)))
                return KL

        def skaggsInfo(self, ratemap, dwelltimes):
                &#39;&#39;&#39;
                Calculates Skaggs information measure

                Parameters
                ----------
                ratemap : numpy.ndarray
                dwelltimes: numpy.ndarray
                        Must be same size as ratemap

                Returns
                -------
                bits_per_spike : float
                        Skaggs information score

                Notes
                -----
                Returns Skaggs et al&#39;s estimate of spatial information in bits per spike:
                        NB THIS DATA SHOULD UNDERGO ADAPTIVE BINNING - See adaptiveBin in binning class above
                I = sum_x p(x) r(x) log(r(x)/r)
                divided by mean rate over bins to get bits per spike
                Inputs:
                        array of firing rates and dwell times per bin.
                Outputs:
                        bits per spike
                binning could be over any single spatial variable (e.g. location, direction, speed).
                &#39;&#39;&#39;

                dwelltimes = dwelltimes / 50 # assumed sample rate of 50Hz
                if np.shape(ratemap) &gt; 1:
                        ratemap = np.reshape(ratemap,(np.prod(np.shape(ratemap)),1))
                        dwelltimes = np.reshape(dwelltimes,(np.prod(np.shape(dwelltimes)),1))
                duration = np.nansum(dwelltimes)
                meanrate = np.nansum(ratemap * dwelltimes) / duration
                if meanrate &lt;= 0.0:
                        bits_per_spike = np.nan
                        return bits_per_spike
                p_x = dwelltimes / duration
                p_r = ratemap / meanrate
                dum = p_x * ratemap
                ind = np.nonzero(dum)[0]
                bits_per_spike = np.nansum(dum[ind] * np.log2(p_r[ind]))
                bits_per_spike = bits_per_spike / meanrate
                return bits_per_spike</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ephysiopy.dacq2py.fieldcalcs.FieldCalcs"><code class="flex name class">
<span>class <span class="ident">FieldCalcs</span></span>
</code></dt>
<dd>
<section class="desc"><p>A series of methods are defined here that allow quantification of ratemaps</p>
<h2 id="methods">Methods</h2>
<ul>
<li>_blur_image(im, n, ny=None, ftype='boxcar')</li>
<li>limit_to_one(rmap, prc=50, min_dist=5)</li>
<li>global_threshold</li>
<li>local_threshold</li>
<li>get_field_props</li>
<li>coherence(smoothed_rate, unsmoothed rate)</li>
<li>kldiv_dir(polarplot)</li>
<li>kldiv(X, pvect1,pvect2,variant=None)</li>
<li>skaggsInfo(ratemap, dwelltimes)</li>
<li>xPearson(ratemap1,ratemap2=None,mode='full')</li>
<li>linearStackAverage(ratemap)</li>
</ul></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FieldCalcs:
        &#39;&#39;&#39;
        A series of methods are defined here that allow quantification of ratemaps
        Methods:
                - _blur_image(im, n, ny=None, ftype=&#39;boxcar&#39;)
                - limit_to_one(rmap, prc=50, min_dist=5)
                - global_threshold
                - local_threshold
                - get_field_props
                - coherence(smoothed_rate, unsmoothed rate)
                - kldiv_dir(polarplot)
                - kldiv(X, pvect1,pvect2,variant=None)
                - skaggsInfo(ratemap, dwelltimes)
                - xPearson(ratemap1,ratemap2=None,mode=&#39;full&#39;)
                - linearStackAverage(ratemap)
        &#39;&#39;&#39;
        def _blur_image(self, im, n, ny=None, ftype=&#39;boxcar&#39;):
                &#34;&#34;&#34; blurs the image by convolving with a filter (&#39;gaussian&#39; or
                        &#39;boxcar&#39;) of
                        size n. The optional keyword argument ny allows for a different
                        size in the y direction.
                &#34;&#34;&#34;
                n = int(n)
                if not ny:
                        ny = n
                else:
                        ny = int(ny)
                #  keep track of nans
                nan_idx = np.isnan(im)
                im[nan_idx] = 0
                if ftype == &#39;boxcar&#39;:
                        if np.ndim(im) == 1:
                                g = signal.boxcar(n) / float(n)
                        elif np.ndim(im) == 2:
                                g = signal.boxcar([n, ny]) / float(n)
                elif ftype == &#39;gaussian&#39;:
                        x, y = np.mgrid[-n:n+1, -ny:ny+1]
                        g = np.exp(-(x**2/float(n) + y**2/float(ny)))
                        g = g / g.sum()
                        if np.ndim(im) == 1:
                                g = g[n, :]
                improc = signal.convolve(im, g, mode=&#39;same&#39;)
                improc[nan_idx] = np.nan
                return improc    
        
        def limit_to_one(self, A, prc=50, min_dist=5):
                &#34;&#34;&#34;
                Processes a multi-peaked ratemap (ie grid cell) and returns a matrix
                where the multi-peaked ratemap consist of a single peaked field that is
                a) not connected to the border and b) close to the middle of the ratemap
                &#34;&#34;&#34;
                Ac = A.copy()
                Ac[np.isnan(A)] = 0
                # smooth Ac more to remove local irregularities
                n = ny = 5
                x, y = np.mgrid[-n:n+1, -ny:ny+1]
                g = np.exp(-(x**2/float(n) + y**2/float(ny)))
                g = g / g.sum()
                Ac = signal.convolve(Ac, g, mode=&#39;same&#39;)
                peak_mask = skimage.feature.peak_local_max(Ac, min_distance=min_dist,
                                                                                                   exclude_border=False,
                                                                                                   indices=False)
                peak_labels = skimage.measure.label(peak_mask, 8)
                field_labels = skimage.morphology.watershed(image=-Ac,
                                                                                                        markers=peak_labels)
                nFields = np.max(field_labels)
                sub_field_mask = np.zeros((nFields, Ac.shape[0], Ac.shape[1]))
                labelled_sub_field_mask = np.zeros_like(sub_field_mask)
                sub_field_props = skimage.measure.regionprops(field_labels,
                                                                                                          intensity_image=Ac)
                sub_field_centroids = []
                sub_field_size = []

                for sub_field in sub_field_props:
                        tmp = np.zeros(Ac.shape).astype(bool)
                        tmp[sub_field.coords[:, 0], sub_field.coords[:, 1]] = True
                        tmp2 = Ac &gt; sub_field.max_intensity * (prc/float(100))
                        sub_field_mask[sub_field.label - 1, :, :] = np.logical_and(tmp2, tmp)
                        labelled_sub_field_mask[sub_field.label-1, np.logical_and(tmp2, tmp)] = sub_field.label
                        sub_field_centroids.append(sub_field.centroid)
                        sub_field_size.append(sub_field.area)  # in bins
                sub_field_mask = np.sum(sub_field_mask, 0)
                middle = np.round(np.array(A.shape) / 2)
                normd_dists = sub_field_centroids - middle
                field_dists_from_middle = np.hypot(normd_dists[:, 0], normd_dists[:, 1])
                central_field_idx = np.argmin(field_dists_from_middle)
                central_field = np.squeeze(labelled_sub_field_mask[central_field_idx, :, :])
                # collapse the labelled mask down to an 2d array
                labelled_sub_field_mask = np.sum(labelled_sub_field_mask, 0)
                # clear the border
                cleared_mask = skimage.segmentation.clear_border(central_field)
                # check we&#39;ve still got stuff in the matrix or fail
                if ~np.any(cleared_mask):
                        print(&#39;No fields were detected away from edges so nothing returned&#39;)
                        return None, None, None
                else:
                        central_field_props = sub_field_props[central_field_idx]
                        return central_field_props, central_field, central_field_idx

        def global_threshold(self, A, prc=50, min_dist=5):
                &#39;&#39;&#39;
                Globally thresholds a ratemap and counts number of fields found
                &#39;&#39;&#39;
                Ac = A.copy()
                Ac[np.isnan(A)] = 0
                n = ny = 5
                x, y = np.mgrid[-n:n+1, -ny:ny+1]
                g = np.exp(-(x**2/float(n) + y**2/float(ny)))
                g = g / g.sum()
                Ac = signal.convolve(Ac, g, mode=&#39;same&#39;)
                maxRate = np.nanmax(np.ravel(Ac))
                Ac[Ac &lt; maxRate*(prc/float(100))] = 0
                peak_mask = skimage.feature.peak_local_max(Ac, min_distance=min_dist,
                                                                                                   exclude_border=False,
                                                                                                   indices=False)
                peak_labels = skimage.measure.label(peak_mask, 8)
                field_labels = skimage.morphology.watershed(image=-Ac,
                                                                                                        markers=peak_labels)
                nFields = np.max(field_labels)
                return nFields

        def local_threshold(self, A, prc=50, min_dist=5):
                &#34;&#34;&#34;
                Locally thresholds a ratemap to take only the surrounding prc amount
                around any local peak
                &#34;&#34;&#34;
                Ac = A.copy()
                nanidx = np.isnan(Ac)
                Ac[nanidx] = 0
                # smooth Ac more to remove local irregularities
                n = ny = 5
                x, y = np.mgrid[-n:n+1, -ny:ny+1]
                g = np.exp(-(x**2/float(n) + y**2/float(ny)))
                g = g / g.sum()
                Ac = signal.convolve(Ac, g, mode=&#39;same&#39;)
                peak_mask = skimage.feature.peak_local_max(Ac, min_distance=min_dist,
                                                                                                   exclude_border=False,
                                                                                                   indices=False)
                peak_labels = skimage.measure.label(peak_mask, 8)
                field_labels = skimage.morphology.watershed(image=-Ac,
                                                                                                        markers=peak_labels)
                nFields = np.max(field_labels)
                sub_field_mask = np.zeros((nFields, Ac.shape[0], Ac.shape[1]))
                sub_field_props = skimage.measure.regionprops(field_labels,
                                                                                                          intensity_image=Ac)
                sub_field_centroids = []
                sub_field_size = []

                for sub_field in sub_field_props:
                        tmp = np.zeros(Ac.shape).astype(bool)
                        tmp[sub_field.coords[:, 0], sub_field.coords[:, 1]] = True
                        tmp2 = Ac &gt; sub_field.max_intensity * (prc/float(100))
                        sub_field_mask[sub_field.label - 1, :, :] = np.logical_and(tmp2, tmp)
                        sub_field_centroids.append(sub_field.centroid)
                        sub_field_size.append(sub_field.area)  # in bins
                sub_field_mask = np.sum(sub_field_mask, 0)
                A_out = np.zeros_like(A)
                A_out[sub_field_mask.astype(bool)] = A[sub_field_mask.astype(bool)]
                A_out[nanidx] = np.nan
                return A_out

        def getBorderScore(self, A, B=None, shape=&#39;square&#39;, fieldThresh=0.3, smthKernSig=3,
                                        circumPrc=0.2, binSize=3.0, minArea=200, debug=False):
                &#39;&#39;&#39;
                Calculates a border score totally dis-similar to that calculated in Solstad et al
                (2008)

                Parameters
                ----------
                A : np.array
                        Should be the ratemap
                B : np.array
                        This should be a boolean mask where True (1)
                        is equivalent to the presence of a border and False (0)
                        is equivalent to &#39;open space&#39;. Naievely this will be the 
                        edges of the ratemap but could be used to take account of 
                        boundary insertions/ creations to check tuning to multiple
                        environmental boundaries. Default None: when the mask is 
                        None then a mask is created that has 1&#39;s at the edges of the
                        ratemap i.e. it is assumed that occupancy = environmental 
                        shape
                shape : str
                        description of environment shape. Currently
                        only &#39;square&#39; or &#39;circle&#39; accepted. Used to calculate the
                        proportion of the environmental boundaries to examine for
                        firing
                fieldThresh : float
                        Between 0 and 1 this is the percentage 
                        amount of the maximum firing rate
                        to remove from the ratemap (i.e. to remove noise)
                smthKernSig : float
                        the sigma value used in smoothing the ratemap
                        (again!) with a gaussian kernel
                circumPrc : float
                        The percentage amount of the circumference
                        of the environment that the field needs to be to count
                        as long enough to make it through
                binSize : float
                        bin size in cm
                minArea : float
                        min area for a field to be considered
                debug : bool
                        If True then some plots and text will be output

                Returns
                -------
                        float : the border score

                Notes
                -----
                If the cell is a border cell (BVC) then we know that it should
                fire at a fixed distance from a given boundary (possibly more
                than one). In essence this algorithm estimates the amount of 
                variance in this distance i.e. if the cell is a border cell this
                number should be small. This is achieved by first doing a bunch of 
                morphological operations to isolate individual fields in the
                ratemap (similar to the code used in phasePrecession.py - see 
                the partitionFields method therein). These partitioned fields are then
                thinned out (using skimage&#39;s skeletonize) to a single pixel
                wide field which will lie more or less in the middle of the
                (highly smoothed) sub-field. It is the variance in distance from the
                nearest boundary along this pseudo-iso-line that is the boundary measure

                Other things to note are that the pixel-wide field has to have some minimum
                length. In the case of a circular environment this is set to 
                20% of the circumference; in the case of a square environment markers
                this is at least half the length of the longest side

                &#39;&#39;&#39;

                dwell = np.isfinite(A)
                # need to know borders of the environment so we can see if a field
                # touches the edges, and the perimeter length of the environment
                # deal with square or circles differently
                borderMask = np.zeros_like(A)
                A_rows, A_cols = np.shape(A)
                if &#39;circle&#39; in shape:
                        radius = np.max(np.array(np.shape(A))) / 2.0
                        dist_mask = skimage.morphology.disk(radius)
                        if np.shape(dist_mask) &gt; np.shape(A):
                                dist_mask = dist_mask[1:A_rows+1, 1:A_cols+1]
                        tmp = np.zeros([A_rows + 2, A_cols + 2])
                        tmp[1:-1, 1:-1] = dist_mask
                        dists = ndimage.morphology.distance_transform_bf(tmp)
                        dists = dists[1:-1, 1:-1]
                        perimeter =  2.0 * radius * np.pi
                        borderMask = np.logical_xor(dists &lt;= 0, dists &lt; 2)
                        # open up the border mask a little
                        borderMask = skimage.morphology.binary_dilation(borderMask, skimage.morphology.disk(1))
                elif &#39;square&#39; in shape:
                        perimeter = np.sum(np.array(np.shape(A)*2))
                        borderMask[0:3, :] = 1
                        borderMask[-3:, :] = 1
                        borderMask[:, 0:3] = 1
                        borderMask[:, -3:] = 1
                        tmp = np.zeros([A_rows + 2, A_cols + 2])
                        dist_mask = np.ones_like(A)
                        tmp[1:-1, 1:-1] = dist_mask
                        dists = ndimage.morphology.distance_transform_bf(tmp)
                        # remove edges to make same shape as input ratemap
                        dists = dists[1:-1, 1:-1]
                A[np.isnan(A)] = 0
                # get some morphological info about the fields in the ratemap
                # start image processing:
                # get some markers
                # NB I&#39;ve tried a variety of techniques to optimise this part and the
                # best seems to be the local adaptive thresholding technique which)
                # smooths locally with a gaussian - see the skimage docs for more
                idx = A &gt;= np.nanmax(np.ravel(A)) * fieldThresh         
                A_thresh = np.zeros_like(A)
                A_thresh[idx] = A[idx]

                # label these markers so each blob has a unique id
                labels, nFields = ndimage.label(A_thresh)
                # remove small objects
                min_size = int(minArea / binSize) - 1
                if debug:
                        plt.figure()
                        plt.imshow(A_thresh)
                        ax = plt.gca()
                        ax.set_title(&#39;Before removing small objects&#39;)
                skimage.morphology.remove_small_objects(labels, min_size=min_size, connectivity=2, in_place=True)
                labels = skimage.segmentation.relabel_sequential(labels)[0]
                nFields = np.max(labels)
                if nFields == 0:
                        return np.nan
                # Iterate over the labelled parts of the array labels calculating
                # how much of the total circumference of the environment edge it
                # covers

                fieldAngularCoverage = np.zeros([1, nFields]) * np.nan
                fractionOfPixelsOnBorder = np.zeros([1, nFields]) * np.nan
                fieldsToKeep = np.zeros_like(A)
                for i in range(1, nFields+1):
                        fieldMask = np.logical_and(labels==i, borderMask)

                        # check the angle subtended by the fieldMask
                        if np.sum(fieldMask.astype(int)) &gt; 0:
                                s = skimage.measure.regionprops(fieldMask.astype(int), intensity_image=A_thresh)[0]
                                x = s.coords[:,0] - (A_cols / 2.0)
                                y = s.coords[:,1] - (A_rows / 2.0)
                                subtended_angle = np.rad2deg(np.ptp(np.arctan2(x,y)))
                                if subtended_angle &gt; (360 * circumPrc):
                                        pixelsOnBorder = np.count_nonzero(fieldMask) / float(np.count_nonzero(labels==i))
                                        fractionOfPixelsOnBorder[:,i-1] = pixelsOnBorder
                                        if pixelsOnBorder &gt; 0.5:
                                                fieldAngularCoverage[0, i-1] = subtended_angle

                                fieldsToKeep = np.logical_or(fieldsToKeep, labels==i)
                if debug:
                        fig, ax = plt.subplots(4,1,figsize=(3,9))
                        ax1 = ax[0]
                        ax2 = ax[1]
                        ax3 = ax[2]
                        ax4 = ax[3]
                        ax1.imshow(A)
                        ax2.imshow(labels)
                        ax3.imshow(A_thresh)
                        ax4.imshow(fieldsToKeep)
                        plt.show()
                        for i,f in enumerate(fieldAngularCoverage.ravel()):
                                print(&#34;angle subtended by field {0} = {1:.2f}&#34;.format(i+1, f))
                        for i,f in enumerate(fractionOfPixelsOnBorder.ravel()):
                                print(&#34;% pixels on border for field {0} = {1:.2f}&#34;.format(i+1, f))
                fieldAngularCoverage = (fieldAngularCoverage / 360.)
                if np.sum(fieldsToKeep) == 0:
                        return np.nan
                rateInField = A[fieldsToKeep]
                # normalize firing rate in the field to sum to 1
                rateInField = rateInField / np.nansum(rateInField)
                dist2WallInField = dists[fieldsToKeep]
                Dm = np.dot(dist2WallInField, rateInField)
                if &#39;circle&#39; in shape:
                        Dm = Dm / radius
                elif &#39;square&#39; in shape:
                        Dm = Dm / (np.max(np.shape(A)) / 2.0)
                borderScore = (fractionOfPixelsOnBorder-Dm) / (fractionOfPixelsOnBorder+Dm)
                return np.max(borderScore)

        def get_field_props(self, A, min_dist=5, neighbours=2, prc=50,
                                                plot=False, ax=None, tri=False, verbose=True, **kwargs):
                &#34;&#34;&#34;
                Returns a dictionary of properties of the field(s) in a ratemap A
                Parameters
                ----------
                A : numpy.array
                        a ratemap (but could be any image)
                min_dist : float
                        the separation (in bins) between fields for measures
                        such as field distance to make sense. Used to
                        partition the image into separate fields in the call to
                        skimage.feature.peak_local_max
                neighbours : int
                        the number of fields to consider as neighbours to
                        any given field. Defaults to 2
                prc : float
                        percent of fields to consider
                ax : matplotlib.Axes
                        user supplied axis. If None a new figure window is created
                tri : bool
                        whether to do Delaunay triangulation between fields
                        and add to plot
                verbose : bool
                        dumps the properties to the console
                plot : bool
                        whether to plot some output - currently consists of the
                        ratemap A, the fields of which are outline in a black
                        contour. Default False

                Returns
                -------
                result : dict
                        The properties of the field(s) in the input ratemap A
                &#34;&#34;&#34;

                from scipy.spatial import Delaunay
                from skimage.measure import find_contours
                from sklearn.neighbors import NearestNeighbors
                import gridcell
                import matplotlib.cm as cm
                nan_idx = np.isnan(A)
                Ac = A.copy()
                Ac[np.isnan(A)] = 0
                # smooth Ac more to remove local irregularities
                n = ny = 5
                x, y = np.mgrid[-n:n+1, -ny:ny+1]
                g = np.exp(-(x**2/float(n) + y**2/float(ny)))
                g = g / g.sum()
                Ac = signal.convolve(Ac, g, mode=&#39;same&#39;)
                if &#39;clear_border&#39; in kwargs.keys():
                        clear_border = True
                else:
                        clear_border = False
                peak_idx = skimage.feature.peak_local_max(Ac, min_distance=min_dist,
                                                                                                  exclude_border=clear_border,
                                                                                                  indices=True)
                if neighbours &gt; len(peak_idx):
                        print(&#39;neighbours value of {0} &gt; the {1} peaks found&#39;.format(neighbours, len(peak_idx)))
                        print(&#39;Reducing neighbours to number of peaks found&#39;)
                        neighbours = len(peak_idx)
                peak_mask = skimage.feature.peak_local_max(Ac, min_distance=min_dist, exclude_border=clear_border,
                                                                                                   indices=False)
                peak_labels = skimage.measure.label(peak_mask, 8)
                field_labels = skimage.morphology.watershed(image=-Ac,
                                                                                                        markers=peak_labels)
                nFields = np.max(field_labels)
                sub_field_mask = np.zeros((nFields, Ac.shape[0], Ac.shape[1]))
                sub_field_props = skimage.measure.regionprops(field_labels,
                                                                                                          intensity_image=Ac)
                sub_field_centroids = []
                sub_field_size = []

                for sub_field in sub_field_props:
                        tmp = np.zeros(Ac.shape).astype(bool)
                        tmp[sub_field.coords[:, 0], sub_field.coords[:, 1]] = True
                        tmp2 = Ac &gt; sub_field.max_intensity * (prc/float(100))
                        sub_field_mask[sub_field.label - 1, :, :] = np.logical_and(tmp2, tmp)
                        sub_field_centroids.append(sub_field.centroid)
                        sub_field_size.append(sub_field.area)  # in bins
                sub_field_mask = np.sum(sub_field_mask, 0)
                contours = skimage.measure.find_contours(sub_field_mask, 0.5)
                # find the nearest neighbors to the peaks of each sub-field


                nbrs = NearestNeighbors(n_neighbors=neighbours, algorithm=&#39;ball_tree&#39;).fit(peak_idx)
                distances, indices = nbrs.kneighbors(peak_idx)
                mean_field_distance = np.mean(distances[:, 1:neighbours])


                nValid_bins = np.sum(~nan_idx)
                # calculate the amount of out of field firing
                A_non_field = np.zeros_like(A) * np.nan
                A_non_field[~sub_field_mask.astype(bool)] = A[~sub_field_mask.astype(bool)]
                A_non_field[nan_idx] = np.nan
                out_of_field_firing_prc = (np.count_nonzero(A_non_field &gt; 0) / float(nValid_bins)) * 100
                Ac[np.isnan(A)] = np.nan
                &#34;&#34;&#34;
                get some stats about the field ellipticity
                &#34;&#34;&#34;
                central_field_props, central_field, central_field_idx = self.limit_to_one(A, prc=50)
                if central_field is None:
                        ellipse_ratio = np.nan
                else:
                        contour_coords = find_contours(central_field, 0.5)
                        G = gridcell.SAC()
                        a = G.__fit_ellipse__(contour_coords[0][:,0], contour_coords[0][:,1])
                        ellipse_axes = G.__ellipse_axis_length__(a)
                        ellipse_ratio = np.min(ellipse_axes) / np.max(ellipse_axes)
                &#39;&#39;&#39; using the peak_idx values calculate the angles of the triangles that
                make up a delaunay tesselation of the space if the calc_angles arg is
                in kwargs
                &#39;&#39;&#39;
                if &#39;calc_angs&#39; in kwargs.keys():
                        try:
                                angs = self.calc_angs(peak_idx)
                        except:
                                angs = np.nan
                else:
                        angs = None

                if plot:
                        if ax is None:
                                fig = plt.figure()
                                ax = fig.add_subplot(111)
                        else:
                                ax = ax
                        Am = np.ma.MaskedArray(Ac, mask=nan_idx, copy=True)
                        ax.pcolormesh(Am, cmap=cm.jet, edgecolors=&#39;face&#39;)
                        for c in contours:
                                ax.plot(c[:, 1], c[:, 0], &#39;k&#39;)
                        # do the delaunay magic
                        if tri:
                                tri = Delaunay(peak_idx)
                                ax.triplot(peak_idx[:,1], peak_idx[:,0], tri.simplices.copy(), color=&#39;w&#39;, marker=&#39;o&#39;)
                        ax.set_xlim(0, Ac.shape[1] - 0.5)
                        ax.set_ylim(0, Ac.shape[0] - 0.5)
                        ax.set_xticklabels(&#39;&#39;)
                        ax.set_yticklabels(&#39;&#39;)
                        ax.invert_yaxis()
                props = {&#39;Ac&#39; : Ac,
                                 &#39;Peak_rate&#39;: np.nanmax(A),
                                 &#39;Mean_rate&#39;: np.nanmean(A),
                                 &#39;Field_size&#39;: np.mean(sub_field_size),
                                 &#39;Pct_bins_with_firing&#39;: (np.sum(sub_field_mask) / nValid_bins) * 100,
                                 &#39;Out_of_field_firing_prc&#39;: out_of_field_firing_prc,
                                 &#39;Dist_between_fields&#39;: mean_field_distance,
                                 &#39;Num_fields&#39;: float(nFields),
                                 &#39;Sub_field_mask&#39;: sub_field_mask,
                                 &#39;Smoothed_map&#39;: Ac,
                                 &#39;field_labels&#39;: field_labels,
                                 &#39;Peak_idx&#39;: peak_idx,
                                 &#39;angles&#39;: angs,
                                 &#39;contours&#39;: contours,
                                 &#39;ellipse_ratio&#39;: ellipse_ratio}

                if verbose:
                        print(&#39;\nPercentage of bins with firing: {:.2%}&#39;.format(np.sum(sub_field_mask) / nValid_bins))
                        print(&#39;Percentage out of field firing: {:.2%}&#39;.format(np.count_nonzero(A_non_field &gt; 0) / float(nValid_bins)))
                        print(&#39;Peak firing rate: {:.3} Hz&#39;.format(np.nanmax(A)))
                        print(&#39;Mean firing rate: {:.3} Hz&#39;.format(np.nanmean(A)))
                        print(&#39;Number of fields: {0}&#39;.format(nFields))
                        print(&#39;Mean field size: {:.5} cm&#39;.format(np.mean(sub_field_size)))  # 3 is binsize)
                        print(&#39;Mean inter-peak distance between fields: {:.4} cm&#39;.format(mean_field_distance))
                return props

        def calc_angs(self, points):
                &#34;&#34;&#34;
                Calculates the angles for all triangles in a delaunay tesselation of
                the peak points in the ratemap
                &#34;&#34;&#34;

                # calculate the lengths of the sides of the triangles
                sideLen = np.hypot(points[:, 0], points[:, 1])
                tri = spatial.Delaunay(points)
                indices, indptr = tri.vertex_neighbor_vertices
                nTris = tri.nsimplex
                outAngs = []
                for k in range(nTris):
                        idx = indptr[indices[k]:indices[k+1]]
                        a = sideLen[k]
                        b = sideLen[idx[0]]
                        c = sideLen[idx[1]]
                        angA = self._getAng(a, b, c)
                        angB = self._getAng(b, c, a)
                        angC = self._getAng(c, a, b)
                        outAngs.append((angA, angB, angC))
                return np.array(outAngs).T

        def _getAng(self, a, b, c):
                &#39;&#39;&#39;
                Given lengths a,b,c of the sides of a triangle this returns the angles
                in degress of all 3 angles
                &#39;&#39;&#39;
                return np.degrees(np.arccos((c**2 - b**2 - a**2)/(-2.0 * a * b)))

        def corr_maps(self, map1, map2, maptype=&#39;normal&#39;):
                &#39;&#39;&#39;
                correlates two ratemaps together ignoring areas that have zero sampling
                &#39;&#39;&#39;
                if map1.shape &gt; map2.shape:
                        map2 = misc.imresize(map2, map1.shape, interp=&#39;nearest&#39;, mode=&#39;F&#39;)
                elif map1.shape &lt; map2.shape:
                        map1 = misc.imresize(map1, map2.shape, interp=&#39;nearest&#39;, mode=&#39;F&#39;)
                map1 = map1.flatten()
                map2 = map2.flatten()
                if maptype is &#39;normal&#39;:
                        valid_map1 = np.logical_or((map1 &gt; 0), ~np.isnan(map1))
                        valid_map2 = np.logical_or((map2 &gt; 0), ~np.isnan(map2))
                elif maptype is &#39;grid&#39;:
                        valid_map1 = ~np.isnan(map1)
                        valid_map2 = ~np.isnan(map2)
                valid = np.logical_and(valid_map1, valid_map2)
                r = np.corrcoef(map1[valid], map2[valid])
                if r.any():
                        return r[1][0]
                else:
                        return np.nan

        def coherence(self, smthd_rate, unsmthd_rate):
                &#39;&#39;&#39;calculates coherence of receptive field via correlation of smoothed
                and unsmoothed ratemaps
                &#39;&#39;&#39;
                smthd = smthd_rate.ravel()
                unsmthd = unsmthd_rate.ravel()
                si = ~np.isnan(smthd)
                ui = ~np.isnan(unsmthd)
                idx = ~(~si | ~ui)
                coherence = np.corrcoef(unsmthd[idx], smthd[idx])
                return coherence[1,0]

        def kldiv_dir(self, polarPlot):
                &#34;&#34;&#34;
                Returns a kl divergence for directional firing: measure of directionality.
                Calculates kl diveregence between a smoothed ratemap (probably should be smoothed
                otherwise information theoretic measures don&#39;t &#39;care&#39; about position of bins relative to
                one another) and a pure circular distribution. The larger the divergence the more
                tendancy the cell has to fire when the animal faces a specific direction.

                Parameters
                ----------
                polarPlot: 1D-array
                        The binned and smoothed directional ratemap

                Returns
                -------
                klDivergence: float
                        The divergence from circular of the 1D-array from a uniform circular
                        distribution
                &#34;&#34;&#34;

                __inc = 0.00001
                polarPlot = np.atleast_2d(polarPlot)
                polarPlot[np.isnan(polarPlot)] = __inc
                polarPlot[polarPlot == 0] = __inc
                normdPolar = polarPlot / float(np.nansum(polarPlot))
                nDirBins = polarPlot.shape[1]
                compCirc = np.ones_like(polarPlot) / float(nDirBins)
                kldivergence = self.kldiv(np.arange(0,nDirBins), normdPolar, compCirc)
                return kldivergence

        def kldiv(self, X, pvect1, pvect2, variant=None):
                &#39;&#39;&#39;
                Calculates the Kullback-Leibler or Jensen-Shannon divergence between two distributions.

                kldiv(X,P1,P2) returns the Kullback-Leibler divergence between two
                distributions specified over the M variable values in vector X.  P1 is a
                length-M vector of probabilities representing distribution 1, and P2 is a
                length-M vector of probabilities representing distribution 2.  Thus, the
                probability of value X(i) is P1(i) for distribution 1 and P2(i) for
                distribution 2.  The Kullback-Leibler divergence is given by:

                .. math:: KL(P1(x),P2(x)) = sum_[P1(x).log(P1(x)/P2(x))]

                If X contains duplicate values, there will be an warning message, and these
                values will be treated as distinct values.  (I.e., the actual values do
                not enter into the computation, but the probabilities for the two
                duplicate values will be considered as probabilities corresponding to
                two unique values.)  The elements of probability vectors P1 and P2 must
                each sum to 1 +/- .00001.

                kldiv(X,P1,P2,&#39;sym&#39;) returns a symmetric variant of the Kullback-Leibler
                divergence, given by [KL(P1,P2)+KL(P2,P1)]/2 [1]_

                kldiv(X,P1,P2,&#39;js&#39;) returns the Jensen-Shannon divergence, given by
                [KL(P1,Q)+KL(P2,Q)]/2, where Q = (P1+P2)/2.  See the Wikipedia article
                for &#34;Kullbackâ€“Leibler divergence&#34;.  This is equal to 1/2 the so-called
                &#34;Jeffrey divergence.&#34; [2]_

                References
                ----------
                .. [1] Johnson, D.H. and S. Sinanovic. &#34;Symmetrizing the Kullback-Leibler
                distance.&#34; IEEE Transactions on Information Theory (Submitted).
                .. [2] Rubner, Y., Tomasi, C., and Guibas, L. J., 2000. &#34;The Earth Mover&#39;s
                distance as a metric for image retrieval.&#34; International Journal of
                Computer Vision, 40(2): 99-121.

                See Also
                --------
                Cover, T.M. and J.A. Thomas. &#34;Elements of Information Theory,&#34; Wiley, 1991.

                https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence

                Notes
                -----
                This function is taken from one on the Mathworks file exchange
                &#39;&#39;&#39;

                if not np.equal(np.unique(X), np.sort(X)).all():
                        warnings.warn(&#39;X contains duplicate values. Treated as distinct values.&#39;,
                                                  UserWarning)
                if not np.equal(np.shape(X), np.shape(pvect1)).all() or not np.equal(np.shape(X), np.shape(pvect2)).all():
                        warnings.warn(&#39;All inputs must have the same dimension.&#39;, UserWarning)
                if (np.abs(np.sum(pvect1) - 1) &gt; 0.00001) or (np.abs(np.sum(pvect2) - 1) &gt; 0.00001):
                        warnings.warn(&#39;Probabilities don&#39;&#39;t sum to 1.&#39;, UserWarning)
                if variant:
                        if variant == &#39;js&#39;:
                                logqvect = np.log2((pvect2 + pvect1) / 2)
                                KL = 0.5 * (np.nansum(pvect1 * (np.log2(pvect1) - logqvect)) + np.sum(pvect2 * (np.log2(pvect2) - logqvect)))
                                return KL
                        elif variant == &#39;sym&#39;:
                                KL1 = np.nansum(pvect1 * (np.log2(pvect1) - np.log2(pvect2)))
                                KL2 = np.nansum(pvect2 * (np.log2(pvect2) - np.log2(pvect1)))
                                KL = (KL1 + KL2) / 2
                                return KL
                        else:
                                warnings.warn(&#39;Last argument not recognised&#39;, UserWarning)
                KL = np.nansum(pvect1 * (np.log2(pvect1) - np.log2(pvect2)))
                return KL

        def skaggsInfo(self, ratemap, dwelltimes):
                &#39;&#39;&#39;
                Calculates Skaggs information measure

                Parameters
                ----------
                ratemap : numpy.ndarray
                dwelltimes: numpy.ndarray
                        Must be same size as ratemap

                Returns
                -------
                bits_per_spike : float
                        Skaggs information score

                Notes
                -----
                Returns Skaggs et al&#39;s estimate of spatial information in bits per spike:
                        NB THIS DATA SHOULD UNDERGO ADAPTIVE BINNING - See adaptiveBin in binning class above
                I = sum_x p(x) r(x) log(r(x)/r)
                divided by mean rate over bins to get bits per spike
                Inputs:
                        array of firing rates and dwell times per bin.
                Outputs:
                        bits per spike
                binning could be over any single spatial variable (e.g. location, direction, speed).
                &#39;&#39;&#39;

                dwelltimes = dwelltimes / 50 # assumed sample rate of 50Hz
                if np.shape(ratemap) &gt; 1:
                        ratemap = np.reshape(ratemap,(np.prod(np.shape(ratemap)),1))
                        dwelltimes = np.reshape(dwelltimes,(np.prod(np.shape(dwelltimes)),1))
                duration = np.nansum(dwelltimes)
                meanrate = np.nansum(ratemap * dwelltimes) / duration
                if meanrate &lt;= 0.0:
                        bits_per_spike = np.nan
                        return bits_per_spike
                p_x = dwelltimes / duration
                p_r = ratemap / meanrate
                dum = p_x * ratemap
                ind = np.nonzero(dum)[0]
                bits_per_spike = np.nansum(dum[ind] * np.log2(p_r[ind]))
                bits_per_spike = bits_per_spike / meanrate
                return bits_per_spike</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.calc_angs"><code class="name flex">
<span>def <span class="ident">calc_angs</span></span>(<span>self, points)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the angles for all triangles in a delaunay tesselation of
the peak points in the ratemap</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_angs(self, points):
        &#34;&#34;&#34;
        Calculates the angles for all triangles in a delaunay tesselation of
        the peak points in the ratemap
        &#34;&#34;&#34;

        # calculate the lengths of the sides of the triangles
        sideLen = np.hypot(points[:, 0], points[:, 1])
        tri = spatial.Delaunay(points)
        indices, indptr = tri.vertex_neighbor_vertices
        nTris = tri.nsimplex
        outAngs = []
        for k in range(nTris):
                idx = indptr[indices[k]:indices[k+1]]
                a = sideLen[k]
                b = sideLen[idx[0]]
                c = sideLen[idx[1]]
                angA = self._getAng(a, b, c)
                angB = self._getAng(b, c, a)
                angC = self._getAng(c, a, b)
                outAngs.append((angA, angB, angC))
        return np.array(outAngs).T</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.coherence"><code class="name flex">
<span>def <span class="ident">coherence</span></span>(<span>self, smthd_rate, unsmthd_rate)</span>
</code></dt>
<dd>
<section class="desc"><p>calculates coherence of receptive field via correlation of smoothed
and unsmoothed ratemaps</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def coherence(self, smthd_rate, unsmthd_rate):
        &#39;&#39;&#39;calculates coherence of receptive field via correlation of smoothed
        and unsmoothed ratemaps
        &#39;&#39;&#39;
        smthd = smthd_rate.ravel()
        unsmthd = unsmthd_rate.ravel()
        si = ~np.isnan(smthd)
        ui = ~np.isnan(unsmthd)
        idx = ~(~si | ~ui)
        coherence = np.corrcoef(unsmthd[idx], smthd[idx])
        return coherence[1,0]</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.corr_maps"><code class="name flex">
<span>def <span class="ident">corr_maps</span></span>(<span>self, map1, map2, maptype='normal')</span>
</code></dt>
<dd>
<section class="desc"><p>correlates two ratemaps together ignoring areas that have zero sampling</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def corr_maps(self, map1, map2, maptype=&#39;normal&#39;):
        &#39;&#39;&#39;
        correlates two ratemaps together ignoring areas that have zero sampling
        &#39;&#39;&#39;
        if map1.shape &gt; map2.shape:
                map2 = misc.imresize(map2, map1.shape, interp=&#39;nearest&#39;, mode=&#39;F&#39;)
        elif map1.shape &lt; map2.shape:
                map1 = misc.imresize(map1, map2.shape, interp=&#39;nearest&#39;, mode=&#39;F&#39;)
        map1 = map1.flatten()
        map2 = map2.flatten()
        if maptype is &#39;normal&#39;:
                valid_map1 = np.logical_or((map1 &gt; 0), ~np.isnan(map1))
                valid_map2 = np.logical_or((map2 &gt; 0), ~np.isnan(map2))
        elif maptype is &#39;grid&#39;:
                valid_map1 = ~np.isnan(map1)
                valid_map2 = ~np.isnan(map2)
        valid = np.logical_and(valid_map1, valid_map2)
        r = np.corrcoef(map1[valid], map2[valid])
        if r.any():
                return r[1][0]
        else:
                return np.nan</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.getBorderScore"><code class="name flex">
<span>def <span class="ident">getBorderScore</span></span>(<span>self, A, B=None, shape='square', fieldThresh=0.3, smthKernSig=3, circumPrc=0.2, binSize=3.0, minArea=200, debug=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates a border score totally dis-similar to that calculated in Solstad et al
(2008)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>A</code></strong> :&ensp;<code>np.array</code></dt>
<dd>Should be the ratemap</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>np.array</code></dt>
<dd>This should be a boolean mask where True (1)
is equivalent to the presence of a border and False (0)
is equivalent to 'open space'. Naievely this will be the
edges of the ratemap but could be used to take account of
boundary insertions/ creations to check tuning to multiple
environmental boundaries. Default None: when the mask is
None then a mask is created that has 1's at the edges of the
ratemap i.e. it is assumed that occupancy = environmental
shape</dd>
<dt><strong><code>shape</code></strong> :&ensp;<code>str</code></dt>
<dd>description of environment shape. Currently
only 'square' or 'circle' accepted. Used to calculate the
proportion of the environmental boundaries to examine for
firing</dd>
<dt><strong><code>fieldThresh</code></strong> :&ensp;<code>float</code></dt>
<dd>Between 0 and 1 this is the percentage
amount of the maximum firing rate
to remove from the ratemap (i.e. to remove noise)</dd>
<dt><strong><code>smthKernSig</code></strong> :&ensp;<code>float</code></dt>
<dd>the sigma value used in smoothing the ratemap
(again!) with a gaussian kernel</dd>
<dt><strong><code>circumPrc</code></strong> :&ensp;<code>float</code></dt>
<dd>The percentage amount of the circumference
of the environment that the field needs to be to count
as long enough to make it through</dd>
<dt><strong><code>binSize</code></strong> :&ensp;<code>float</code></dt>
<dd>bin size in cm</dd>
<dt><strong><code>minArea</code></strong> :&ensp;<code>float</code></dt>
<dd>min area for a field to be considered</dd>
<dt><strong><code>debug</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True then some plots and text will be output</dd>
</dl>
<h2 id="returns">Returns</h2>
<pre><code>    float : the border score
</code></pre>
<h2 id="notes">Notes</h2>
<p>If the cell is a border cell (BVC) then we know that it should
fire at a fixed distance from a given boundary (possibly more
than one). In essence this algorithm estimates the amount of
variance in this distance i.e. if the cell is a border cell this
number should be small. This is achieved by first doing a bunch of
morphological operations to isolate individual fields in the
ratemap (similar to the code used in phasePrecession.py - see
the partitionFields method therein). These partitioned fields are then
thinned out (using skimage's skeletonize) to a single pixel
wide field which will lie more or less in the middle of the
(highly smoothed) sub-field. It is the variance in distance from the
nearest boundary along this pseudo-iso-line that is the boundary measure</p>
<p>Other things to note are that the pixel-wide field has to have some minimum
length. In the case of a circular environment this is set to
20% of the circumference; in the case of a square environment markers
this is at least half the length of the longest side</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getBorderScore(self, A, B=None, shape=&#39;square&#39;, fieldThresh=0.3, smthKernSig=3,
                                circumPrc=0.2, binSize=3.0, minArea=200, debug=False):
        &#39;&#39;&#39;
        Calculates a border score totally dis-similar to that calculated in Solstad et al
        (2008)

        Parameters
        ----------
        A : np.array
                Should be the ratemap
        B : np.array
                This should be a boolean mask where True (1)
                is equivalent to the presence of a border and False (0)
                is equivalent to &#39;open space&#39;. Naievely this will be the 
                edges of the ratemap but could be used to take account of 
                boundary insertions/ creations to check tuning to multiple
                environmental boundaries. Default None: when the mask is 
                None then a mask is created that has 1&#39;s at the edges of the
                ratemap i.e. it is assumed that occupancy = environmental 
                shape
        shape : str
                description of environment shape. Currently
                only &#39;square&#39; or &#39;circle&#39; accepted. Used to calculate the
                proportion of the environmental boundaries to examine for
                firing
        fieldThresh : float
                Between 0 and 1 this is the percentage 
                amount of the maximum firing rate
                to remove from the ratemap (i.e. to remove noise)
        smthKernSig : float
                the sigma value used in smoothing the ratemap
                (again!) with a gaussian kernel
        circumPrc : float
                The percentage amount of the circumference
                of the environment that the field needs to be to count
                as long enough to make it through
        binSize : float
                bin size in cm
        minArea : float
                min area for a field to be considered
        debug : bool
                If True then some plots and text will be output

        Returns
        -------
                float : the border score

        Notes
        -----
        If the cell is a border cell (BVC) then we know that it should
        fire at a fixed distance from a given boundary (possibly more
        than one). In essence this algorithm estimates the amount of 
        variance in this distance i.e. if the cell is a border cell this
        number should be small. This is achieved by first doing a bunch of 
        morphological operations to isolate individual fields in the
        ratemap (similar to the code used in phasePrecession.py - see 
        the partitionFields method therein). These partitioned fields are then
        thinned out (using skimage&#39;s skeletonize) to a single pixel
        wide field which will lie more or less in the middle of the
        (highly smoothed) sub-field. It is the variance in distance from the
        nearest boundary along this pseudo-iso-line that is the boundary measure

        Other things to note are that the pixel-wide field has to have some minimum
        length. In the case of a circular environment this is set to 
        20% of the circumference; in the case of a square environment markers
        this is at least half the length of the longest side

        &#39;&#39;&#39;

        dwell = np.isfinite(A)
        # need to know borders of the environment so we can see if a field
        # touches the edges, and the perimeter length of the environment
        # deal with square or circles differently
        borderMask = np.zeros_like(A)
        A_rows, A_cols = np.shape(A)
        if &#39;circle&#39; in shape:
                radius = np.max(np.array(np.shape(A))) / 2.0
                dist_mask = skimage.morphology.disk(radius)
                if np.shape(dist_mask) &gt; np.shape(A):
                        dist_mask = dist_mask[1:A_rows+1, 1:A_cols+1]
                tmp = np.zeros([A_rows + 2, A_cols + 2])
                tmp[1:-1, 1:-1] = dist_mask
                dists = ndimage.morphology.distance_transform_bf(tmp)
                dists = dists[1:-1, 1:-1]
                perimeter =  2.0 * radius * np.pi
                borderMask = np.logical_xor(dists &lt;= 0, dists &lt; 2)
                # open up the border mask a little
                borderMask = skimage.morphology.binary_dilation(borderMask, skimage.morphology.disk(1))
        elif &#39;square&#39; in shape:
                perimeter = np.sum(np.array(np.shape(A)*2))
                borderMask[0:3, :] = 1
                borderMask[-3:, :] = 1
                borderMask[:, 0:3] = 1
                borderMask[:, -3:] = 1
                tmp = np.zeros([A_rows + 2, A_cols + 2])
                dist_mask = np.ones_like(A)
                tmp[1:-1, 1:-1] = dist_mask
                dists = ndimage.morphology.distance_transform_bf(tmp)
                # remove edges to make same shape as input ratemap
                dists = dists[1:-1, 1:-1]
        A[np.isnan(A)] = 0
        # get some morphological info about the fields in the ratemap
        # start image processing:
        # get some markers
        # NB I&#39;ve tried a variety of techniques to optimise this part and the
        # best seems to be the local adaptive thresholding technique which)
        # smooths locally with a gaussian - see the skimage docs for more
        idx = A &gt;= np.nanmax(np.ravel(A)) * fieldThresh         
        A_thresh = np.zeros_like(A)
        A_thresh[idx] = A[idx]

        # label these markers so each blob has a unique id
        labels, nFields = ndimage.label(A_thresh)
        # remove small objects
        min_size = int(minArea / binSize) - 1
        if debug:
                plt.figure()
                plt.imshow(A_thresh)
                ax = plt.gca()
                ax.set_title(&#39;Before removing small objects&#39;)
        skimage.morphology.remove_small_objects(labels, min_size=min_size, connectivity=2, in_place=True)
        labels = skimage.segmentation.relabel_sequential(labels)[0]
        nFields = np.max(labels)
        if nFields == 0:
                return np.nan
        # Iterate over the labelled parts of the array labels calculating
        # how much of the total circumference of the environment edge it
        # covers

        fieldAngularCoverage = np.zeros([1, nFields]) * np.nan
        fractionOfPixelsOnBorder = np.zeros([1, nFields]) * np.nan
        fieldsToKeep = np.zeros_like(A)
        for i in range(1, nFields+1):
                fieldMask = np.logical_and(labels==i, borderMask)

                # check the angle subtended by the fieldMask
                if np.sum(fieldMask.astype(int)) &gt; 0:
                        s = skimage.measure.regionprops(fieldMask.astype(int), intensity_image=A_thresh)[0]
                        x = s.coords[:,0] - (A_cols / 2.0)
                        y = s.coords[:,1] - (A_rows / 2.0)
                        subtended_angle = np.rad2deg(np.ptp(np.arctan2(x,y)))
                        if subtended_angle &gt; (360 * circumPrc):
                                pixelsOnBorder = np.count_nonzero(fieldMask) / float(np.count_nonzero(labels==i))
                                fractionOfPixelsOnBorder[:,i-1] = pixelsOnBorder
                                if pixelsOnBorder &gt; 0.5:
                                        fieldAngularCoverage[0, i-1] = subtended_angle

                        fieldsToKeep = np.logical_or(fieldsToKeep, labels==i)
        if debug:
                fig, ax = plt.subplots(4,1,figsize=(3,9))
                ax1 = ax[0]
                ax2 = ax[1]
                ax3 = ax[2]
                ax4 = ax[3]
                ax1.imshow(A)
                ax2.imshow(labels)
                ax3.imshow(A_thresh)
                ax4.imshow(fieldsToKeep)
                plt.show()
                for i,f in enumerate(fieldAngularCoverage.ravel()):
                        print(&#34;angle subtended by field {0} = {1:.2f}&#34;.format(i+1, f))
                for i,f in enumerate(fractionOfPixelsOnBorder.ravel()):
                        print(&#34;% pixels on border for field {0} = {1:.2f}&#34;.format(i+1, f))
        fieldAngularCoverage = (fieldAngularCoverage / 360.)
        if np.sum(fieldsToKeep) == 0:
                return np.nan
        rateInField = A[fieldsToKeep]
        # normalize firing rate in the field to sum to 1
        rateInField = rateInField / np.nansum(rateInField)
        dist2WallInField = dists[fieldsToKeep]
        Dm = np.dot(dist2WallInField, rateInField)
        if &#39;circle&#39; in shape:
                Dm = Dm / radius
        elif &#39;square&#39; in shape:
                Dm = Dm / (np.max(np.shape(A)) / 2.0)
        borderScore = (fractionOfPixelsOnBorder-Dm) / (fractionOfPixelsOnBorder+Dm)
        return np.max(borderScore)</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.get_field_props"><code class="name flex">
<span>def <span class="ident">get_field_props</span></span>(<span>self, A, min_dist=5, neighbours=2, prc=50, plot=False, ax=None, tri=False, verbose=True, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns a dictionary of properties of the field(s) in a ratemap A
Parameters</p>
<hr>
<dl>
<dt><strong><code>A</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>a ratemap (but could be any image)</dd>
<dt><strong><code>min_dist</code></strong> :&ensp;<code>float</code></dt>
<dd>the separation (in bins) between fields for measures
such as field distance to make sense. Used to
partition the image into separate fields in the call to
skimage.feature.peak_local_max</dd>
<dt><strong><code>neighbours</code></strong> :&ensp;<code>int</code></dt>
<dd>the number of fields to consider as neighbours to
any given field. Defaults to 2</dd>
<dt><strong><code>prc</code></strong> :&ensp;<code>float</code></dt>
<dd>percent of fields to consider</dd>
<dt><strong><code>ax</code></strong> :&ensp;<code>matplotlib.Axes</code></dt>
<dd>user supplied axis. If None a new figure window is created</dd>
<dt><strong><code>tri</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to do Delaunay triangulation between fields
and add to plot</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>dumps the properties to the console</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to plot some output - currently consists of the
ratemap A, the fields of which are outline in a black
contour. Default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>result</code></strong> :&ensp;<code>dict</code></dt>
<dd>The properties of the field(s) in the input ratemap A</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_field_props(self, A, min_dist=5, neighbours=2, prc=50,
                                        plot=False, ax=None, tri=False, verbose=True, **kwargs):
        &#34;&#34;&#34;
        Returns a dictionary of properties of the field(s) in a ratemap A
        Parameters
        ----------
        A : numpy.array
                a ratemap (but could be any image)
        min_dist : float
                the separation (in bins) between fields for measures
                such as field distance to make sense. Used to
                partition the image into separate fields in the call to
                skimage.feature.peak_local_max
        neighbours : int
                the number of fields to consider as neighbours to
                any given field. Defaults to 2
        prc : float
                percent of fields to consider
        ax : matplotlib.Axes
                user supplied axis. If None a new figure window is created
        tri : bool
                whether to do Delaunay triangulation between fields
                and add to plot
        verbose : bool
                dumps the properties to the console
        plot : bool
                whether to plot some output - currently consists of the
                ratemap A, the fields of which are outline in a black
                contour. Default False

        Returns
        -------
        result : dict
                The properties of the field(s) in the input ratemap A
        &#34;&#34;&#34;

        from scipy.spatial import Delaunay
        from skimage.measure import find_contours
        from sklearn.neighbors import NearestNeighbors
        import gridcell
        import matplotlib.cm as cm
        nan_idx = np.isnan(A)
        Ac = A.copy()
        Ac[np.isnan(A)] = 0
        # smooth Ac more to remove local irregularities
        n = ny = 5
        x, y = np.mgrid[-n:n+1, -ny:ny+1]
        g = np.exp(-(x**2/float(n) + y**2/float(ny)))
        g = g / g.sum()
        Ac = signal.convolve(Ac, g, mode=&#39;same&#39;)
        if &#39;clear_border&#39; in kwargs.keys():
                clear_border = True
        else:
                clear_border = False
        peak_idx = skimage.feature.peak_local_max(Ac, min_distance=min_dist,
                                                                                          exclude_border=clear_border,
                                                                                          indices=True)
        if neighbours &gt; len(peak_idx):
                print(&#39;neighbours value of {0} &gt; the {1} peaks found&#39;.format(neighbours, len(peak_idx)))
                print(&#39;Reducing neighbours to number of peaks found&#39;)
                neighbours = len(peak_idx)
        peak_mask = skimage.feature.peak_local_max(Ac, min_distance=min_dist, exclude_border=clear_border,
                                                                                           indices=False)
        peak_labels = skimage.measure.label(peak_mask, 8)
        field_labels = skimage.morphology.watershed(image=-Ac,
                                                                                                markers=peak_labels)
        nFields = np.max(field_labels)
        sub_field_mask = np.zeros((nFields, Ac.shape[0], Ac.shape[1]))
        sub_field_props = skimage.measure.regionprops(field_labels,
                                                                                                  intensity_image=Ac)
        sub_field_centroids = []
        sub_field_size = []

        for sub_field in sub_field_props:
                tmp = np.zeros(Ac.shape).astype(bool)
                tmp[sub_field.coords[:, 0], sub_field.coords[:, 1]] = True
                tmp2 = Ac &gt; sub_field.max_intensity * (prc/float(100))
                sub_field_mask[sub_field.label - 1, :, :] = np.logical_and(tmp2, tmp)
                sub_field_centroids.append(sub_field.centroid)
                sub_field_size.append(sub_field.area)  # in bins
        sub_field_mask = np.sum(sub_field_mask, 0)
        contours = skimage.measure.find_contours(sub_field_mask, 0.5)
        # find the nearest neighbors to the peaks of each sub-field


        nbrs = NearestNeighbors(n_neighbors=neighbours, algorithm=&#39;ball_tree&#39;).fit(peak_idx)
        distances, indices = nbrs.kneighbors(peak_idx)
        mean_field_distance = np.mean(distances[:, 1:neighbours])


        nValid_bins = np.sum(~nan_idx)
        # calculate the amount of out of field firing
        A_non_field = np.zeros_like(A) * np.nan
        A_non_field[~sub_field_mask.astype(bool)] = A[~sub_field_mask.astype(bool)]
        A_non_field[nan_idx] = np.nan
        out_of_field_firing_prc = (np.count_nonzero(A_non_field &gt; 0) / float(nValid_bins)) * 100
        Ac[np.isnan(A)] = np.nan
        &#34;&#34;&#34;
        get some stats about the field ellipticity
        &#34;&#34;&#34;
        central_field_props, central_field, central_field_idx = self.limit_to_one(A, prc=50)
        if central_field is None:
                ellipse_ratio = np.nan
        else:
                contour_coords = find_contours(central_field, 0.5)
                G = gridcell.SAC()
                a = G.__fit_ellipse__(contour_coords[0][:,0], contour_coords[0][:,1])
                ellipse_axes = G.__ellipse_axis_length__(a)
                ellipse_ratio = np.min(ellipse_axes) / np.max(ellipse_axes)
        &#39;&#39;&#39; using the peak_idx values calculate the angles of the triangles that
        make up a delaunay tesselation of the space if the calc_angles arg is
        in kwargs
        &#39;&#39;&#39;
        if &#39;calc_angs&#39; in kwargs.keys():
                try:
                        angs = self.calc_angs(peak_idx)
                except:
                        angs = np.nan
        else:
                angs = None

        if plot:
                if ax is None:
                        fig = plt.figure()
                        ax = fig.add_subplot(111)
                else:
                        ax = ax
                Am = np.ma.MaskedArray(Ac, mask=nan_idx, copy=True)
                ax.pcolormesh(Am, cmap=cm.jet, edgecolors=&#39;face&#39;)
                for c in contours:
                        ax.plot(c[:, 1], c[:, 0], &#39;k&#39;)
                # do the delaunay magic
                if tri:
                        tri = Delaunay(peak_idx)
                        ax.triplot(peak_idx[:,1], peak_idx[:,0], tri.simplices.copy(), color=&#39;w&#39;, marker=&#39;o&#39;)
                ax.set_xlim(0, Ac.shape[1] - 0.5)
                ax.set_ylim(0, Ac.shape[0] - 0.5)
                ax.set_xticklabels(&#39;&#39;)
                ax.set_yticklabels(&#39;&#39;)
                ax.invert_yaxis()
        props = {&#39;Ac&#39; : Ac,
                         &#39;Peak_rate&#39;: np.nanmax(A),
                         &#39;Mean_rate&#39;: np.nanmean(A),
                         &#39;Field_size&#39;: np.mean(sub_field_size),
                         &#39;Pct_bins_with_firing&#39;: (np.sum(sub_field_mask) / nValid_bins) * 100,
                         &#39;Out_of_field_firing_prc&#39;: out_of_field_firing_prc,
                         &#39;Dist_between_fields&#39;: mean_field_distance,
                         &#39;Num_fields&#39;: float(nFields),
                         &#39;Sub_field_mask&#39;: sub_field_mask,
                         &#39;Smoothed_map&#39;: Ac,
                         &#39;field_labels&#39;: field_labels,
                         &#39;Peak_idx&#39;: peak_idx,
                         &#39;angles&#39;: angs,
                         &#39;contours&#39;: contours,
                         &#39;ellipse_ratio&#39;: ellipse_ratio}

        if verbose:
                print(&#39;\nPercentage of bins with firing: {:.2%}&#39;.format(np.sum(sub_field_mask) / nValid_bins))
                print(&#39;Percentage out of field firing: {:.2%}&#39;.format(np.count_nonzero(A_non_field &gt; 0) / float(nValid_bins)))
                print(&#39;Peak firing rate: {:.3} Hz&#39;.format(np.nanmax(A)))
                print(&#39;Mean firing rate: {:.3} Hz&#39;.format(np.nanmean(A)))
                print(&#39;Number of fields: {0}&#39;.format(nFields))
                print(&#39;Mean field size: {:.5} cm&#39;.format(np.mean(sub_field_size)))  # 3 is binsize)
                print(&#39;Mean inter-peak distance between fields: {:.4} cm&#39;.format(mean_field_distance))
        return props</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.global_threshold"><code class="name flex">
<span>def <span class="ident">global_threshold</span></span>(<span>self, A, prc=50, min_dist=5)</span>
</code></dt>
<dd>
<section class="desc"><p>Globally thresholds a ratemap and counts number of fields found</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def global_threshold(self, A, prc=50, min_dist=5):
        &#39;&#39;&#39;
        Globally thresholds a ratemap and counts number of fields found
        &#39;&#39;&#39;
        Ac = A.copy()
        Ac[np.isnan(A)] = 0
        n = ny = 5
        x, y = np.mgrid[-n:n+1, -ny:ny+1]
        g = np.exp(-(x**2/float(n) + y**2/float(ny)))
        g = g / g.sum()
        Ac = signal.convolve(Ac, g, mode=&#39;same&#39;)
        maxRate = np.nanmax(np.ravel(Ac))
        Ac[Ac &lt; maxRate*(prc/float(100))] = 0
        peak_mask = skimage.feature.peak_local_max(Ac, min_distance=min_dist,
                                                                                           exclude_border=False,
                                                                                           indices=False)
        peak_labels = skimage.measure.label(peak_mask, 8)
        field_labels = skimage.morphology.watershed(image=-Ac,
                                                                                                markers=peak_labels)
        nFields = np.max(field_labels)
        return nFields</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.kldiv"><code class="name flex">
<span>def <span class="ident">kldiv</span></span>(<span>self, X, pvect1, pvect2, variant=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the Kullback-Leibler or Jensen-Shannon divergence between two distributions.</p>
<p>kldiv(X,P1,P2) returns the Kullback-Leibler divergence between two
distributions specified over the M variable values in vector X.
P1 is a
length-M vector of probabilities representing distribution 1, and P2 is a
length-M vector of probabilities representing distribution 2.
Thus, the
probability of value X(i) is P1(i) for distribution 1 and P2(i) for
distribution 2.
The Kullback-Leibler divergence is given by:</p>
<p>[
]
If X contains duplicate values, there will be an warning message, and these
values will be treated as distinct values.
(I.e., the actual values do
not enter into the computation, but the probabilities for the two
duplicate values will be considered as probabilities corresponding to
two unique values.)
The elements of probability vectors P1 and P2 must
each sum to 1 +/- .00001.</p>
<p>kldiv(X,P1,P2,'sym') returns a symmetric variant of the Kullback-Leibler
divergence, given by [KL(P1,P2)+KL(P2,P1)]/2 [1]_</p>
<p>kldiv(X,P1,P2,'js') returns the Jensen-Shannon divergence, given by
[KL(P1,Q)+KL(P2,Q)]/2, where Q = (P1+P2)/2.
See the Wikipedia article
for "Kullbackâ€“Leibler divergence".
This is equal to 1/2 the so-called
"Jeffrey divergence." [2]_</p>
<h2 id="references">References</h2>
<p>.. [1] Johnson, D.H. and S. Sinanovic. "Symmetrizing the Kullback-Leibler
distance." IEEE Transactions on Information Theory (Submitted).
.. [2] Rubner, Y., Tomasi, C., and Guibas, L. J., 2000. "The Earth Mover's
distance as a metric for image retrieval." International Journal of
Computer Vision, 40(2): 99-121.</p>
<h2 id="see-also">See Also</h2>
<p><code>Cover</code>, <code>T.M. and J.A. Thomas. "Elements of Information Theory," Wiley</code>, <code>1991.</code></p>
<p><code>&lt;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&gt;</code></p>
<h2 id="notes">Notes</h2>
<p>This function is taken from one on the Mathworks file exchange</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kldiv(self, X, pvect1, pvect2, variant=None):
        &#39;&#39;&#39;
        Calculates the Kullback-Leibler or Jensen-Shannon divergence between two distributions.

        kldiv(X,P1,P2) returns the Kullback-Leibler divergence between two
        distributions specified over the M variable values in vector X.  P1 is a
        length-M vector of probabilities representing distribution 1, and P2 is a
        length-M vector of probabilities representing distribution 2.  Thus, the
        probability of value X(i) is P1(i) for distribution 1 and P2(i) for
        distribution 2.  The Kullback-Leibler divergence is given by:

        .. math:: KL(P1(x),P2(x)) = sum_[P1(x).log(P1(x)/P2(x))]

        If X contains duplicate values, there will be an warning message, and these
        values will be treated as distinct values.  (I.e., the actual values do
        not enter into the computation, but the probabilities for the two
        duplicate values will be considered as probabilities corresponding to
        two unique values.)  The elements of probability vectors P1 and P2 must
        each sum to 1 +/- .00001.

        kldiv(X,P1,P2,&#39;sym&#39;) returns a symmetric variant of the Kullback-Leibler
        divergence, given by [KL(P1,P2)+KL(P2,P1)]/2 [1]_

        kldiv(X,P1,P2,&#39;js&#39;) returns the Jensen-Shannon divergence, given by
        [KL(P1,Q)+KL(P2,Q)]/2, where Q = (P1+P2)/2.  See the Wikipedia article
        for &#34;Kullbackâ€“Leibler divergence&#34;.  This is equal to 1/2 the so-called
        &#34;Jeffrey divergence.&#34; [2]_

        References
        ----------
        .. [1] Johnson, D.H. and S. Sinanovic. &#34;Symmetrizing the Kullback-Leibler
        distance.&#34; IEEE Transactions on Information Theory (Submitted).
        .. [2] Rubner, Y., Tomasi, C., and Guibas, L. J., 2000. &#34;The Earth Mover&#39;s
        distance as a metric for image retrieval.&#34; International Journal of
        Computer Vision, 40(2): 99-121.

        See Also
        --------
        Cover, T.M. and J.A. Thomas. &#34;Elements of Information Theory,&#34; Wiley, 1991.

        https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence

        Notes
        -----
        This function is taken from one on the Mathworks file exchange
        &#39;&#39;&#39;

        if not np.equal(np.unique(X), np.sort(X)).all():
                warnings.warn(&#39;X contains duplicate values. Treated as distinct values.&#39;,
                                          UserWarning)
        if not np.equal(np.shape(X), np.shape(pvect1)).all() or not np.equal(np.shape(X), np.shape(pvect2)).all():
                warnings.warn(&#39;All inputs must have the same dimension.&#39;, UserWarning)
        if (np.abs(np.sum(pvect1) - 1) &gt; 0.00001) or (np.abs(np.sum(pvect2) - 1) &gt; 0.00001):
                warnings.warn(&#39;Probabilities don&#39;&#39;t sum to 1.&#39;, UserWarning)
        if variant:
                if variant == &#39;js&#39;:
                        logqvect = np.log2((pvect2 + pvect1) / 2)
                        KL = 0.5 * (np.nansum(pvect1 * (np.log2(pvect1) - logqvect)) + np.sum(pvect2 * (np.log2(pvect2) - logqvect)))
                        return KL
                elif variant == &#39;sym&#39;:
                        KL1 = np.nansum(pvect1 * (np.log2(pvect1) - np.log2(pvect2)))
                        KL2 = np.nansum(pvect2 * (np.log2(pvect2) - np.log2(pvect1)))
                        KL = (KL1 + KL2) / 2
                        return KL
                else:
                        warnings.warn(&#39;Last argument not recognised&#39;, UserWarning)
        KL = np.nansum(pvect1 * (np.log2(pvect1) - np.log2(pvect2)))
        return KL</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.kldiv_dir"><code class="name flex">
<span>def <span class="ident">kldiv_dir</span></span>(<span>self, polarPlot)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns a kl divergence for directional firing: measure of directionality.
Calculates kl diveregence between a smoothed ratemap (probably should be smoothed
otherwise information theoretic measures don't 'care' about position of bins relative to
one another) and a pure circular distribution. The larger the divergence the more
tendancy the cell has to fire when the animal faces a specific direction.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>polarPlot</code></strong> :&ensp;<code>1D</code>-<code>array</code></dt>
<dd>The binned and smoothed directional ratemap</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>klDivergence</code></strong> :&ensp;<code>float</code></dt>
<dd>The divergence from circular of the 1D-array from a uniform circular
distribution</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kldiv_dir(self, polarPlot):
        &#34;&#34;&#34;
        Returns a kl divergence for directional firing: measure of directionality.
        Calculates kl diveregence between a smoothed ratemap (probably should be smoothed
        otherwise information theoretic measures don&#39;t &#39;care&#39; about position of bins relative to
        one another) and a pure circular distribution. The larger the divergence the more
        tendancy the cell has to fire when the animal faces a specific direction.

        Parameters
        ----------
        polarPlot: 1D-array
                The binned and smoothed directional ratemap

        Returns
        -------
        klDivergence: float
                The divergence from circular of the 1D-array from a uniform circular
                distribution
        &#34;&#34;&#34;

        __inc = 0.00001
        polarPlot = np.atleast_2d(polarPlot)
        polarPlot[np.isnan(polarPlot)] = __inc
        polarPlot[polarPlot == 0] = __inc
        normdPolar = polarPlot / float(np.nansum(polarPlot))
        nDirBins = polarPlot.shape[1]
        compCirc = np.ones_like(polarPlot) / float(nDirBins)
        kldivergence = self.kldiv(np.arange(0,nDirBins), normdPolar, compCirc)
        return kldivergence</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.limit_to_one"><code class="name flex">
<span>def <span class="ident">limit_to_one</span></span>(<span>self, A, prc=50, min_dist=5)</span>
</code></dt>
<dd>
<section class="desc"><p>Processes a multi-peaked ratemap (ie grid cell) and returns a matrix
where the multi-peaked ratemap consist of a single peaked field that is
a) not connected to the border and b) close to the middle of the ratemap</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def limit_to_one(self, A, prc=50, min_dist=5):
        &#34;&#34;&#34;
        Processes a multi-peaked ratemap (ie grid cell) and returns a matrix
        where the multi-peaked ratemap consist of a single peaked field that is
        a) not connected to the border and b) close to the middle of the ratemap
        &#34;&#34;&#34;
        Ac = A.copy()
        Ac[np.isnan(A)] = 0
        # smooth Ac more to remove local irregularities
        n = ny = 5
        x, y = np.mgrid[-n:n+1, -ny:ny+1]
        g = np.exp(-(x**2/float(n) + y**2/float(ny)))
        g = g / g.sum()
        Ac = signal.convolve(Ac, g, mode=&#39;same&#39;)
        peak_mask = skimage.feature.peak_local_max(Ac, min_distance=min_dist,
                                                                                           exclude_border=False,
                                                                                           indices=False)
        peak_labels = skimage.measure.label(peak_mask, 8)
        field_labels = skimage.morphology.watershed(image=-Ac,
                                                                                                markers=peak_labels)
        nFields = np.max(field_labels)
        sub_field_mask = np.zeros((nFields, Ac.shape[0], Ac.shape[1]))
        labelled_sub_field_mask = np.zeros_like(sub_field_mask)
        sub_field_props = skimage.measure.regionprops(field_labels,
                                                                                                  intensity_image=Ac)
        sub_field_centroids = []
        sub_field_size = []

        for sub_field in sub_field_props:
                tmp = np.zeros(Ac.shape).astype(bool)
                tmp[sub_field.coords[:, 0], sub_field.coords[:, 1]] = True
                tmp2 = Ac &gt; sub_field.max_intensity * (prc/float(100))
                sub_field_mask[sub_field.label - 1, :, :] = np.logical_and(tmp2, tmp)
                labelled_sub_field_mask[sub_field.label-1, np.logical_and(tmp2, tmp)] = sub_field.label
                sub_field_centroids.append(sub_field.centroid)
                sub_field_size.append(sub_field.area)  # in bins
        sub_field_mask = np.sum(sub_field_mask, 0)
        middle = np.round(np.array(A.shape) / 2)
        normd_dists = sub_field_centroids - middle
        field_dists_from_middle = np.hypot(normd_dists[:, 0], normd_dists[:, 1])
        central_field_idx = np.argmin(field_dists_from_middle)
        central_field = np.squeeze(labelled_sub_field_mask[central_field_idx, :, :])
        # collapse the labelled mask down to an 2d array
        labelled_sub_field_mask = np.sum(labelled_sub_field_mask, 0)
        # clear the border
        cleared_mask = skimage.segmentation.clear_border(central_field)
        # check we&#39;ve still got stuff in the matrix or fail
        if ~np.any(cleared_mask):
                print(&#39;No fields were detected away from edges so nothing returned&#39;)
                return None, None, None
        else:
                central_field_props = sub_field_props[central_field_idx]
                return central_field_props, central_field, central_field_idx</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.local_threshold"><code class="name flex">
<span>def <span class="ident">local_threshold</span></span>(<span>self, A, prc=50, min_dist=5)</span>
</code></dt>
<dd>
<section class="desc"><p>Locally thresholds a ratemap to take only the surrounding prc amount
around any local peak</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def local_threshold(self, A, prc=50, min_dist=5):
        &#34;&#34;&#34;
        Locally thresholds a ratemap to take only the surrounding prc amount
        around any local peak
        &#34;&#34;&#34;
        Ac = A.copy()
        nanidx = np.isnan(Ac)
        Ac[nanidx] = 0
        # smooth Ac more to remove local irregularities
        n = ny = 5
        x, y = np.mgrid[-n:n+1, -ny:ny+1]
        g = np.exp(-(x**2/float(n) + y**2/float(ny)))
        g = g / g.sum()
        Ac = signal.convolve(Ac, g, mode=&#39;same&#39;)
        peak_mask = skimage.feature.peak_local_max(Ac, min_distance=min_dist,
                                                                                           exclude_border=False,
                                                                                           indices=False)
        peak_labels = skimage.measure.label(peak_mask, 8)
        field_labels = skimage.morphology.watershed(image=-Ac,
                                                                                                markers=peak_labels)
        nFields = np.max(field_labels)
        sub_field_mask = np.zeros((nFields, Ac.shape[0], Ac.shape[1]))
        sub_field_props = skimage.measure.regionprops(field_labels,
                                                                                                  intensity_image=Ac)
        sub_field_centroids = []
        sub_field_size = []

        for sub_field in sub_field_props:
                tmp = np.zeros(Ac.shape).astype(bool)
                tmp[sub_field.coords[:, 0], sub_field.coords[:, 1]] = True
                tmp2 = Ac &gt; sub_field.max_intensity * (prc/float(100))
                sub_field_mask[sub_field.label - 1, :, :] = np.logical_and(tmp2, tmp)
                sub_field_centroids.append(sub_field.centroid)
                sub_field_size.append(sub_field.area)  # in bins
        sub_field_mask = np.sum(sub_field_mask, 0)
        A_out = np.zeros_like(A)
        A_out[sub_field_mask.astype(bool)] = A[sub_field_mask.astype(bool)]
        A_out[nanidx] = np.nan
        return A_out</code></pre>
</details>
</dd>
<dt id="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.skaggsInfo"><code class="name flex">
<span>def <span class="ident">skaggsInfo</span></span>(<span>self, ratemap, dwelltimes)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates Skaggs information measure</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ratemap</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>dwelltimes</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Must be same size as ratemap</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>bits_per_spike</code></strong> :&ensp;<code>float</code></dt>
<dd>Skaggs information score</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>Returns Skaggs et al's estimate of spatial information in bits per spike:
NB THIS DATA SHOULD UNDERGO ADAPTIVE BINNING - See adaptiveBin in binning class above
I = sum_x p(x) r(x) log(r(x)/r)
divided by mean rate over bins to get bits per spike</p>
<h2 id="inputs">Inputs</h2>
<p>array of firing rates and dwell times per bin.</p>
<h2 id="outputs">Outputs</h2>
<p>bits per spike
binning could be over any single spatial variable (e.g. location, direction, speed).</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def skaggsInfo(self, ratemap, dwelltimes):
        &#39;&#39;&#39;
        Calculates Skaggs information measure

        Parameters
        ----------
        ratemap : numpy.ndarray
        dwelltimes: numpy.ndarray
                Must be same size as ratemap

        Returns
        -------
        bits_per_spike : float
                Skaggs information score

        Notes
        -----
        Returns Skaggs et al&#39;s estimate of spatial information in bits per spike:
                NB THIS DATA SHOULD UNDERGO ADAPTIVE BINNING - See adaptiveBin in binning class above
        I = sum_x p(x) r(x) log(r(x)/r)
        divided by mean rate over bins to get bits per spike
        Inputs:
                array of firing rates and dwell times per bin.
        Outputs:
                bits per spike
        binning could be over any single spatial variable (e.g. location, direction, speed).
        &#39;&#39;&#39;

        dwelltimes = dwelltimes / 50 # assumed sample rate of 50Hz
        if np.shape(ratemap) &gt; 1:
                ratemap = np.reshape(ratemap,(np.prod(np.shape(ratemap)),1))
                dwelltimes = np.reshape(dwelltimes,(np.prod(np.shape(dwelltimes)),1))
        duration = np.nansum(dwelltimes)
        meanrate = np.nansum(ratemap * dwelltimes) / duration
        if meanrate &lt;= 0.0:
                bits_per_spike = np.nan
                return bits_per_spike
        p_x = dwelltimes / duration
        p_r = ratemap / meanrate
        dum = p_x * ratemap
        ind = np.nonzero(dum)[0]
        bits_per_spike = np.nansum(dum[ind] * np.log2(p_r[ind]))
        bits_per_spike = bits_per_spike / meanrate
        return bits_per_spike</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ephysiopy.dacq2py" href="index.html">ephysiopy.dacq2py</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ephysiopy.dacq2py.fieldcalcs.FieldCalcs" href="#ephysiopy.dacq2py.fieldcalcs.FieldCalcs">FieldCalcs</a></code></h4>
<ul class="two-column">
<li><code><a title="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.calc_angs" href="#ephysiopy.dacq2py.fieldcalcs.FieldCalcs.calc_angs">calc_angs</a></code></li>
<li><code><a title="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.coherence" href="#ephysiopy.dacq2py.fieldcalcs.FieldCalcs.coherence">coherence</a></code></li>
<li><code><a title="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.corr_maps" href="#ephysiopy.dacq2py.fieldcalcs.FieldCalcs.corr_maps">corr_maps</a></code></li>
<li><code><a title="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.getBorderScore" href="#ephysiopy.dacq2py.fieldcalcs.FieldCalcs.getBorderScore">getBorderScore</a></code></li>
<li><code><a title="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.get_field_props" href="#ephysiopy.dacq2py.fieldcalcs.FieldCalcs.get_field_props">get_field_props</a></code></li>
<li><code><a title="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.global_threshold" href="#ephysiopy.dacq2py.fieldcalcs.FieldCalcs.global_threshold">global_threshold</a></code></li>
<li><code><a title="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.kldiv" href="#ephysiopy.dacq2py.fieldcalcs.FieldCalcs.kldiv">kldiv</a></code></li>
<li><code><a title="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.kldiv_dir" href="#ephysiopy.dacq2py.fieldcalcs.FieldCalcs.kldiv_dir">kldiv_dir</a></code></li>
<li><code><a title="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.limit_to_one" href="#ephysiopy.dacq2py.fieldcalcs.FieldCalcs.limit_to_one">limit_to_one</a></code></li>
<li><code><a title="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.local_threshold" href="#ephysiopy.dacq2py.fieldcalcs.FieldCalcs.local_threshold">local_threshold</a></code></li>
<li><code><a title="ephysiopy.dacq2py.fieldcalcs.FieldCalcs.skaggsInfo" href="#ephysiopy.dacq2py.fieldcalcs.FieldCalcs.skaggsInfo">skaggsInfo</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.5</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>